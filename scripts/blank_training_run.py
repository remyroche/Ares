#!/usr/bin/env python3
"""
Script to run a 'blank' training pipeline with minimal optimization parameters.

This is intended for testing the end-to-end functionality of the training pipeline
without performing a full, computationally intensive hyperparameter optimization.
It overrides CONFIG values for optimization trials and data lookback to be very small.

Usage:
    python scripts/blank_training_run.py --symbol BTCUSDT --exchange BINANCE
"""

import asyncio
import sys
import time
from datetime import datetime
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import necessary modules
import argparse

from src.config import CONFIG  # Import the global CONFIG dictionary
from src.database.sqlite_manager import SQLiteManager
from src.training.enhanced_training_manager import EnhancedTrainingManager
from src.utils.logger import setup_logging, system_logger


async def main():
    """
    Main function to orchestrate the blank training run.
    It temporarily modifies CONFIG parameters for quick execution.
    """
    start_time = time.time()
    setup_logging()
    logger = system_logger.getChild("BlankTrainingRun")

    logger.info("=" * 80)
    logger.info("ğŸš€ BLANK TRAINING RUN INITIALIZATION")
    logger.info("=" * 80)
    logger.info(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Python version: {sys.version}")
    logger.info(f"Working directory: {Path.cwd()}")
    logger.info(f"Script path: {Path(__file__).absolute()}")

    parser = argparse.ArgumentParser(
        description="Run a 'blank' training pipeline for testing.",
    )
    parser.add_argument(
        "--symbol",
        type=str,
        default=CONFIG.get("SYMBOL", "BTCUSDT"),
        help="The trading symbol for the blank run (e.g., BTCUSDT).",
    )
    parser.add_argument(
        "--exchange",
        type=str,
        default=CONFIG.get("EXCHANGE", "BINANCE"),
        help="The exchange for the blank run (e.g., BINANCE).",
    )
    args = parser.parse_args()

    logger.info("ğŸ“‹ Command line arguments:")
    logger.info(f"   Symbol: {args.symbol}")
    logger.info(f"   Exchange: {args.exchange}")

    logger.info(
        f"ğŸš€ Starting 'blank' training pipeline for {args.symbol} on {args.exchange}...",
    )
    print(
        f"ğŸš€ Starting 'blank' training pipeline for {args.symbol} on {args.exchange}...",
    )
    logger.info("Temporarily overriding CONFIG parameters for quick test run.")
    print("Temporarily overriding CONFIG parameters for quick test run.")

    # For a blank run, we reduce the number of optimization trials to make it run faster.
    # NOTE: The data lookback is now passed as a parameter to the training manager.
    CONFIG["MODEL_TRAINING"]["hyperparameter_tuning"]["max_trials"] = (
        3  # For Optuna in base_ensemble
    )
    CONFIG["MODEL_TRAINING"]["coarse_hpo"] = {
        "n_trials": 3,
    }  # For CoarseOptimizer's HPO

    # Set blank training mode flag for NaN handling
    CONFIG["BLANK_TRAINING_MODE"] = True

    db_manager = None
    try:
        logger.info("ğŸ”§ Initializing SQLiteManager...")
        print("ğŸ”§ Initializing SQLiteManager...")
        # Initialize SQLiteManager
        db_manager = SQLiteManager({})
        await db_manager.initialize()
        logger.info("âœ… SQLiteManager initialized successfully")
        print("âœ… SQLiteManager initialized successfully")

        logger.info("ğŸ”§ Initializing TrainingManager...")
        print("ğŸ”§ Initializing TrainingManager...")
        # Instantiate EnhancedTrainingManager
        training_manager = EnhancedTrainingManager(db_manager)
        logger.info("âœ… TrainingManager initialized successfully")
        print("âœ… TrainingManager initialized successfully")

        logger.info("ğŸš€ Starting full training pipeline...")
        print("ğŸš€ Starting full training pipeline...")
        training_start_time = time.time()

        # Skip data downloading and use existing data only
        logger.info("â­ï¸  Skipping data downloading - using existing data only")
        print("â­ï¸  Skipping data downloading - using existing data only")

        # Check if required data files exist (using actual file patterns)
        klines_filename = f"data_cache/klines_{args.exchange}_{args.symbol}_1m_consolidated_fixed.csv"  # Use fixed 1m consolidated
        agg_trades_filename = f"data_cache/aggtrades_{args.exchange}_{args.symbol}_2025-07-31.csv"  # Use most recent
        futures_filename = (
            f"data_cache/futures_{args.exchange}_{args.symbol}_consolidated.csv"
        )

        # Check if all required data files exist
        import os

        required_files = [klines_filename, agg_trades_filename, futures_filename]
        missing_files = [f for f in required_files if not os.path.exists(f)]

        if missing_files:
            logger.error(f"âŒ Missing required data files: {missing_files}")
            logger.error(
                "Please run data downloading first or ensure data files exist.",
            )
            logger.error("Expected files:")
            for file in required_files:
                logger.error(f"  - {file}")
            sys.exit(1)

        logger.info("âœ… All required data files found, proceeding with training")
        print("âœ… All required data files found, proceeding with training")

        # Run the training pipeline stages manually, skipping data collection
        logger.info(
            "ğŸš€ Running training pipeline stages manually (skipping data collection)",
        )
        print("ğŸš€ Running training pipeline stages manually (skipping data collection)")
        logger.info(
            f"   Parameters: symbol={args.symbol}, exchange={args.exchange}, lookback_days_override=60",
        )
        print(
            f"   Parameters: symbol={args.symbol}, exchange={args.exchange}, lookback_days_override=60",
        )

        try:
            print("ğŸ”„ Starting training pipeline stages...")

            # Stage 1: Setup training environment
            logger.info("ğŸ”§ Stage 1: Setting up training environment...")
            print("ğŸ”§ Stage 1: Setting up training environment...")
            setup_success = await training_manager._setup_training_environment(
                args.symbol,
                args.exchange,
                "1m",
            )
            if not setup_success:
                raise Exception("Stage 1 (Setup) failed")
            logger.info("âœ… Stage 1 completed successfully")
            print("âœ… Stage 1 completed successfully")

            # Use existing data file instead of collecting new data
            csv_data_file = f"data_cache/klines_{args.exchange}_{args.symbol}_1m_consolidated_fixed.csv"
            logger.info(f"ğŸ“ Using existing CSV data file: {csv_data_file}")
            print(f"ğŸ“ Using existing CSV data file: {csv_data_file}")

            # Use the existing CSV formatting script to ensure proper data format
            logger.info(
                "ğŸ”„ Using CSV formatting script to ensure proper data format...",
            )
            print("ğŸ”„ Using CSV formatting script to ensure proper data format...")

            # Import and run the CSV formatting script
            from src.training.aggtrades_data_formatting import (
                auto_reformat_aggtrades_files_for_exchange,
            )

            # Run the CSV formatting to ensure data files for this exchange/symbol are in correct format
            logger.info("ğŸ“‹ Running CSV format validation and conversion...")
            print("ğŸ“‹ Running CSV format validation and conversion...")
            auto_reformat_aggtrades_files_for_exchange(args.exchange, args.symbol)

            # Create the required pickle file from existing CSV data
            logger.info("ğŸ”„ Creating pickle file from existing CSV data...")
            print("ğŸ”„ Creating pickle file from existing CSV data...")

            import os
            import pickle

            import pandas as pd

            # Read the CSV data (now properly formatted)
            if not os.path.exists(csv_data_file):
                raise Exception(f"CSV data file not found: {csv_data_file}")

            klines_df = pd.read_csv(csv_data_file)
            logger.info(
                f"ğŸ“Š Loaded CSV data: {len(klines_df)} rows, {len(klines_df.columns)} columns",
            )
            print(
                f"ğŸ“Š Loaded CSV data: {len(klines_df)} rows, {len(klines_df.columns)} columns",
            )

            # Add quality validation checks
            logger.info("ğŸ” Running data quality validation...")
            print("ğŸ” Running data quality validation...")
            
            from src.training.training_validation_config import validate_data_format, validate_data_quality
            
            # Load additional data for validation
            agg_trades_file = f"data_cache/aggtrades_{args.exchange}_{args.symbol}_consolidated.csv"
            futures_file = f"data_cache/futures_{args.exchange}_{args.symbol}_consolidated.csv"
            
            validation_data = {"klines": klines_df}
            
            # Load aggregated trades if available
            if os.path.exists(agg_trades_file):
                agg_trades_df = pd.read_csv(agg_trades_file)
                validation_data["agg_trades"] = agg_trades_df
                logger.info(f"ğŸ“Š Loaded agg trades: {len(agg_trades_df)} rows")
            else:
                logger.warning("âš ï¸  Aggregated trades file not found, skipping validation")
            
            # Load futures data if available
            if os.path.exists(futures_file):
                futures_df = pd.read_csv(futures_file)
                validation_data["futures"] = futures_df
                logger.info(f"ğŸ“Š Loaded futures: {len(futures_df)} rows")
            else:
                logger.warning("âš ï¸  Futures file not found, skipping validation")
            
            # Validate data format
            format_valid, format_errors = validate_data_format(validation_data)
            if not format_valid:
                logger.error(f"âŒ Data format validation failed: {format_errors}")
                print(f"âŒ Data format validation failed: {format_errors}")
                raise Exception("Data format validation failed")
            logger.info("âœ… Data format validation passed")
            print("âœ… Data format validation passed")
            
            # Validate data quality
            quality_valid, quality_errors = validate_data_quality(validation_data)
            if not quality_valid:
                logger.error(f"âŒ Data quality validation failed: {quality_errors}")
                print(f"âŒ Data quality validation failed: {quality_errors}")
                raise Exception("Data quality validation failed")
            logger.info("âœ… Data quality validation passed")
            print("âœ… Data quality validation passed")

            # Create the pickle file in the expected format
            pickle_data = {"klines": klines_df}
            pickle_file = f"data/training/{args.exchange}_{args.symbol}_collected_data.pkl"
            os.makedirs("data/training", exist_ok=True)

            with open(pickle_file, "wb") as f:
                pickle.dump(pickle_data, f)

            logger.info(f"âœ… Created pickle file: {pickle_file}")
            print(f"âœ… Created pickle file: {pickle_file}")

            # Use the pickle file for training
            data_file = pickle_file

            # Stage 3: Model Optimization and Training
            logger.info("ğŸ”§ Stage 3: Optimizing and training models...")
            print("ğŸ”§ Stage 3: Optimizing and training models...")
            training_success = await training_manager._optimize_and_train_models(
                args.symbol,
                "1h",
                data_file,
            )
            if not training_success:
                raise Exception("Stage 3 (Training) failed")
            logger.info("âœ… Stage 3 completed successfully")
            print("âœ… Stage 3 completed successfully")

            # Stage 4: Model Validation and Testing
            logger.info("ğŸ”§ Stage 4: Validating and testing models...")
            print("ğŸ”§ Stage 4: Validating and testing models...")
            validation_success = await training_manager._validate_and_test_models(
                args.symbol,
                data_file,
            )
            if not validation_success:
                raise Exception("Stage 4 (Validation) failed")
            logger.info("âœ… Stage 4 completed successfully")
            print("âœ… Stage 4 completed successfully")

            # Stage 5: Final Setup and Artifact Management
            session_id = f"{args.symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            run_id = "blank_run_" + session_id
            logger.info("ğŸ”§ Stage 5: Finalizing training...")
            print("ğŸ”§ Stage 5: Finalizing training...")
            finalize_success = await training_manager._finalize_training(
                args.symbol,
                session_id,
                run_id,
            )
            if not finalize_success:
                raise Exception("Stage 5 (Finalization) failed")
            logger.info("âœ… Stage 5 completed successfully")
            print("âœ… Stage 5 completed successfully")

            success = True
            logger.info("âœ… All training stages completed successfully")
            print("âœ… All training stages completed successfully")

        except Exception as e:
            logger.error(f"âŒ Training pipeline failed with exception: {e}")
            print(f"âŒ Training pipeline failed with exception: {e}")
            logger.error(f"Exception type: {type(e).__name__}")
            print(f"Exception type: {type(e).__name__}")
            logger.error("Full traceback:")
            print("Full traceback:")
            import traceback

            logger.error(traceback.format_exc())
            print(traceback.format_exc())
            raise

        training_duration = time.time() - training_start_time
        logger.info(
            f"â±ï¸  Training pipeline completed in {training_duration:.2f} seconds",
        )

        if success:
            logger.info("âœ… 'Blank' training pipeline completed successfully!")
            logger.info("ğŸ“Š Training summary:")
            logger.info(f"   Symbol: {args.symbol}")
            logger.info(f"   Exchange: {args.exchange}")
            logger.info(f"   Duration: {training_duration:.2f} seconds")
            logger.info("   Status: SUCCESS")
        else:
            logger.error("âŒ 'Blank' training pipeline failed.")
            logger.error("ğŸ“Š Training summary:")
            logger.error(f"   Symbol: {args.symbol}")
            logger.error(f"   Exchange: {args.exchange}")
            logger.error(f"   Duration: {training_duration:.2f} seconds")
            logger.error("   Status: FAILED")
            sys.exit(1)

    except Exception as e:
        logger.critical("ğŸ’¥ CRITICAL ERROR during blank training run")
        logger.critical(f"Error type: {type(e).__name__}")
        logger.critical(f"Error message: {str(e)}")
        logger.critical("Full traceback:")
        logger.critical(traceback.format_exc())

        # Log additional debugging information
        logger.critical("ğŸ“Š Error context:")
        logger.critical(f"   Symbol: {args.symbol}")
        logger.critical(f"   Exchange: {args.exchange}")
        logger.critical(f"   Python path: {sys.path[:3]}...")  # First 3 entries
        logger.critical(f"   Working directory: {Path.cwd()}")

        sys.exit(1)
    finally:
        # Ensure database connection is closed
        if db_manager and db_manager.conn:
            logger.info("ğŸ”§ Closing database connection...")
            await db_manager.close()
            logger.info("âœ… Database connection closed")

        total_duration = time.time() - start_time
        logger.info("=" * 80)
        logger.info("ğŸ BLANK TRAINING RUN COMPLETION")
        logger.info("=" * 80)
        logger.info(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Total duration: {total_duration:.2f} seconds")
        logger.info(f"Symbol: {args.symbol}")
        logger.info(f"Exchange: {args.exchange}")
        logger.info("ğŸ 'Blank' training pipeline finished.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user (Ctrl+C)")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error in main execution: {e}")
        sys.exit(1)
