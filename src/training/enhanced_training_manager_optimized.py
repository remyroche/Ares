# src/training/enhanced_training_manager_optimized.py

import asyncio
import gc
import multiprocessing as mp
import os
import pickle
import psutil
import random
import warnings
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
# Optional dependency: pyarrow is used for efficient parquet streaming; import lazily
try:
    import pyarrow as pa  # type: ignore
    import pyarrow.parquet as pq  # type: ignore
except Exception:
    pa = None  # type: ignore
    pq = None  # type: ignore
from sklearn.base import BaseEstimator

from src.utils.error_handler import handle_errors, handle_specific_errors
from src.utils.logger import system_logger
from src.utils.validator_orchestrator import validator_orchestrator

# Note: Avoid global warning suppression; use scoped suppression with warnings.catch_warnings in specific call sites if necessary


def _make_hashable(obj: Any) -> Any:
    """Recursively convert potentially unhashable objects (lists, dicts, arrays) into hashable tuples.
    This is used to generate robust cache keys.
    """
    if isinstance(obj, dict):
        return tuple(sorted((k, _make_hashable(v)) for k, v in obj.items()))
    if isinstance(obj, (list, tuple, set)):
        return tuple(_make_hashable(v) for v in obj)
    if isinstance(obj, np.ndarray):
        return tuple(obj.tolist())
    return obj


class CachedBacktester:
    """Cached backtesting to avoid redundant calculations."""
    
    def __init__(self, market_data: pd.DataFrame):
        self.market_data = market_data
        self.cache = {}
        self.technical_indicators = self._precompute_indicators()
        self.logger = system_logger.getChild("CachedBacktester")
    
    def _precompute_indicators(self) -> Dict[str, np.ndarray]:
        """Precompute all technical indicators once."""
        indicators: Dict[str, np.ndarray] = {}
        
        if 'close' not in self.market_data.columns:
            self.logger.warning("'close' column missing; cannot compute indicators")
            return indicators
        
        # Precompute common indicators
        indicators['sma_20'] = (
            self.market_data['close'].rolling(20).mean().fillna(0).values
        )
        indicators['sma_50'] = (
            self.market_data['close'].rolling(50).mean().fillna(0).values
        )
        indicators['ema_12'] = self.market_data['close'].ewm(span=12).mean().fillna(0).values
        indicators['ema_26'] = self.market_data['close'].ewm(span=26).mean().fillna(0).values
        
        # RSI calculation with zero-loss guard
        delta = self.market_data['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / (loss.replace(0, np.nan))
        indicators['rsi'] = (100 - (100 / (1 + rs))).fillna(50).values
        
        # ATR calculation only if required columns exist
        if {'high', 'low'}.issubset(self.market_data.columns):
            high_low = self.market_data['high'] - self.market_data['low']
            high_close = np.abs(self.market_data['high'] - self.market_data['close'].shift())
            low_close = np.abs(self.market_data['low'] - self.market_data['close'].shift())
            tr = np.maximum(high_low.values, np.maximum(high_close.values, low_close.values))
            indicators['atr'] = (
                pd.Series(tr, index=self.market_data.index).rolling(window=14).mean().fillna(0).values
            )
        else:
            self.logger.warning("Missing 'high'/'low' columns; skipping ATR calculation")
        
        # Volatility
        indicators['volatility'] = (
            self.market_data['close'].pct_change().rolling(20).std().fillna(0).values
        )
        
        # Volume indicators
        if 'volume' in self.market_data.columns:
            indicators['volume_sma'] = self.market_data['volume'].rolling(20).mean().fillna(0).values
        
        self.logger.info(f"Precomputed {len(indicators)} technical indicators")
        return indicators
    
    def run_cached_backtest(self, params: Dict[str, Any]) -> float:
        """Run backtest using cached indicators."""
        cache_key = self._generate_cache_key(params)
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Run simplified backtest using precomputed indicators
        result = self._run_simplified_backtest(params)
        self.cache[cache_key] = result
        return result
    
    def _generate_cache_key(self, params: Dict[str, Any]) -> str:
        """Generate cache key from parameters, robust to unhashable values."""
        return str(hash(_make_hashable(params)))
    
    def _run_simplified_backtest(self, params: Dict[str, Any]) -> float:
        """Run simplified backtest logic."""
        # This is a placeholder - implement your actual backtesting logic
        # using the precomputed indicators
        return random.uniform(-1.0, 1.0)  # Placeholder return


class ProgressiveEvaluator:
    """Progressive evaluation to stop unpromising trials early."""
    
    def __init__(self, full_data: pd.DataFrame):
        self.full_data = full_data
        self.evaluation_stages = [
            (0.1, 0.3),   # 10% data, 30% weight
            (0.3, 0.5),   # 30% data, 50% weight  
            (1.0, 1.0)    # 100% data, 100% weight
        ]
        self.logger = system_logger.getChild("ProgressiveEvaluator")
    
    def evaluate_progressively(self, params: Dict[str, Any], 
                             evaluator_func) -> float:
        """Evaluate parameters progressively across data subsets."""
        total_score = 0
        total_weight = 0
        
        for data_ratio, weight in self.evaluation_stages:
            subset_size = int(len(self.full_data) * data_ratio)
            subset_data = self.full_data.iloc[:subset_size]
            
            score = evaluator_func(subset_data, params)
            total_score += score * weight
            total_weight += weight
            
            # Early stopping if performance is poor
            if data_ratio < 1.0 and score < -0.5:
                self.logger.info(f"Early stopping at {data_ratio*100}% data due to poor performance")
                return -1.0  # Stop evaluation
        
        return total_score / total_weight


class ParallelBacktester:
    """Parallel backtesting for multiple parameter combinations."""
    
    def __init__(self, n_workers: int = None):
        self.n_workers = n_workers or min(mp.cpu_count(), 8)
        self.executor = ProcessPoolExecutor(max_workers=self.n_workers)
        self.logger = system_logger.getChild("ParallelBacktester")
    
    def evaluate_batch(self, param_batch: List[Dict[str, Any]], 
                      market_data: pd.DataFrame) -> List[float]:
        """Evaluate multiple parameter sets in parallel."""
        
        # Prepare data for parallel processing
        data_pickle = pickle.dumps(market_data)
        
        # Submit batch for parallel evaluation
        futures = []
        for params in param_batch:
            future = self.executor.submit(
                self._evaluate_single_params, 
                data_pickle, 
                params
            )
            futures.append(future)
        
        # Collect results
        results = [future.result() for future in futures]
        self.logger.info(f"Evaluated {len(results)} parameter sets in parallel")
        return results
    
    @staticmethod
    def _evaluate_single_params(data_pickle: bytes, params: Dict[str, Any]) -> float:
        """Evaluate single parameter set (runs in separate process)."""
        market_data = pickle.loads(data_pickle)
        # Implement your evaluation logic here
        return random.uniform(-1.0, 1.0)  # Placeholder
    
    def __del__(self):
        """Clean up executor."""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)


class IncrementalTrainer:
    """Incremental training to reuse model states."""
    
    def __init__(self, base_model_config: Dict[str, Any]):
        self.base_config = base_model_config
        self.model_cache = {}
        self.logger = system_logger.getChild("IncrementalTrainer")
    
    def train_incrementally(self, params: Dict[str, Any], 
                          X: np.ndarray, y: np.ndarray) -> Any:
        """Train model incrementally from cached state."""
        
        # Generate model key based on core parameters
        model_key = self._generate_model_key(params)
        
        if model_key in self.model_cache:
            # Continue training from cached state
            model = self.model_cache[model_key]
            self.logger.info("Continuing training from cached model state")
            # Note: Implementation depends on your specific model type
        else:
            # Train new model
            model = self._create_model(params)
            self.logger.info("Training new model")
            self.model_cache[model_key] = model
        
        return model
    
    def _generate_model_key(self, params: Dict[str, Any]) -> str:
        """Generate cache key based on core model parameters."""
        core_params = {
            'max_depth': params.get('max_depth'),
            'learning_rate': params.get('learning_rate'),
            'subsample': params.get('subsample'),
            'colsample_bytree': params.get('colsample_bytree')
        }
        return str(hash(_make_hashable(core_params)))
    
    def _create_model(self, params: Dict[str, Any]) -> Any:
        """Create new model with given parameters."""
        # Placeholder - implement your model creation logic
        return None


class StreamingDataProcessor:
    """Streaming processor for large datasets."""
    
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
        self.logger = system_logger.getChild("StreamingDataProcessor")
    
    def process_data_stream(self, data_path: str) -> pd.DataFrame:
        """Process data in streaming fashion."""
        try:
            if data_path.endswith('.parquet'):
                return self._process_parquet_stream(data_path)
            elif data_path.endswith('.csv'):
                return self._process_csv_stream(data_path)
            else:
                raise ValueError(f"Unsupported file format: {data_path}")
        except Exception as e:
            self.logger.error(f"Error processing data stream: {e}")
            raise
    
    def _process_parquet_stream(self, file_path: str) -> pd.DataFrame:
        """Process Parquet file in chunks."""
        chunks: List[pd.DataFrame] = []
        
        if pq is None:
            self.logger.warning("pyarrow not available; falling back to pandas read_parquet")
            try:
                return pd.read_parquet(file_path)
            except FileNotFoundError as e:
                self.logger.error(f"Parquet file not found: {file_path}")
                raise
            except Exception as e:
                self.logger.error(f"Error reading Parquet with pandas: {e}")
                raise
        
        try:
            parquet_file = pq.ParquetFile(file_path)
            for batch in parquet_file.iter_batches(batch_size=self.chunk_size):
                chunk = batch.to_pandas()
                # Process chunk if needed
                chunks.append(chunk)
            
            self.logger.info(f"Processed {len(chunks)} chunks from Parquet file")
            return pd.concat(chunks, ignore_index=True)
        except FileNotFoundError:
            self.logger.error(f"Parquet file not found: {file_path}")
            raise
        except Exception as e:
            # Provide better diagnostics for common pyarrow errors when available
            if pa is not None and isinstance(e, pa.lib.ArrowInvalid):  # type: ignore[attr-defined]
                self.logger.error(f"Invalid Parquet file format: {e}")
            else:
                self.logger.error(f"Error processing Parquet stream: {e}")
            raise
    
    def _process_csv_stream(self, file_path: str) -> pd.DataFrame:
        """Process CSV file in chunks."""
        chunks: List[pd.DataFrame] = []
        
        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):
            # Process chunk if needed
            chunks.append(chunk)
        
        self.logger.info(f"Processed {len(chunks)} chunks from CSV file")
        return pd.concat(chunks, ignore_index=True)


class AdaptiveSampler:
    """Adaptive sampling to focus on promising regions."""
    
    def __init__(self, initial_samples: int = 100):
        self.initial_samples = initial_samples
        self.promising_regions = []
        self.trial_history = []
        self.logger = system_logger.getChild("AdaptiveSampler")
    
    def suggest_parameters(self, parameter_bounds: Dict[str, Tuple[float, float]]) -> Dict[str, Any]:
        """Suggest parameters based on promising regions."""
        
        if len(self.trial_history) < self.initial_samples:
            # Random sampling for initial exploration
            return self._random_sampling(parameter_bounds)
        else:
            # Focus on promising regions
            return self._adaptive_sampling(parameter_bounds)
    
    def update_trial_history(self, params: Dict[str, Any], score: float):
        """Update trial history with new result."""
        self.trial_history.append({'params': params, 'score': score})
    
    def _adaptive_sampling(self, parameter_bounds: Dict[str, Tuple[float, float]]) -> Dict[str, Any]:
        """Sample from promising regions identified in history."""
        
        # Identify promising regions (top 25% of trials)
        sorted_trials = sorted(self.trial_history, key=lambda x: x['score'], reverse=True)
        top_quartile = sorted_trials[:len(sorted_trials)//4]
        
        if not top_quartile:
            return self._random_sampling(parameter_bounds)
        
        # Sample around good trials with some noise
        reference_trial = random.choice(top_quartile)
        return self._perturb_parameters(reference_trial['params'], parameter_bounds)
    
    def _random_sampling(self, parameter_bounds: Dict[str, Tuple[float, float]]) -> Dict[str, Any]:
        """Random parameter sampling."""
        params = {}
        for param_name, (min_val, max_val) in parameter_bounds.items():
            params[param_name] = random.uniform(min_val, max_val)
        return params
    
    def _perturb_parameters(self, base_params: Dict[str, Any], 
                           parameter_bounds: Dict[str, Tuple[float, float]]) -> Dict[str, Any]:
        """Perturb parameters around promising region."""
        perturbed = {}
        perturbation_factor = 0.1  # 10% perturbation
        
        for param_name, base_value in base_params.items():
            if param_name in parameter_bounds:
                min_val, max_val = parameter_bounds[param_name]
                range_val = max_val - min_val
                noise = random.uniform(-perturbation_factor, perturbation_factor) * range_val
                new_value = np.clip(base_value + noise, min_val, max_val)
                perturbed[param_name] = new_value
            else:
                perturbed[param_name] = base_value
        
        return perturbed


class MemoryEfficientDataManager:
    """Memory-efficient data structures for large datasets."""
    
    def __init__(self):
        self.logger = system_logger.getChild("MemoryEfficientDataManager")
        self.data_cache = {}
    
    def optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Optimize DataFrame for memory usage."""
        
        # Use appropriate dtypes
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
        
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
        
        # Convert object columns to category if appropriate
        for col in df.select_dtypes(include=['object']).columns:
            if df[col].nunique() / len(df) < 0.5:  # If less than 50% unique values
                df[col] = df[col].astype('category')
        
        self.logger.info(f"Optimized DataFrame memory usage")
        return df
    
    def save_to_parquet(self, df: pd.DataFrame, file_path: str, 
                       compression: str = 'snappy') -> None:
        """Save DataFrame to Parquet format for efficient storage."""
        df.to_parquet(file_path, compression=compression, index=False)
        self.logger.info(f"Saved DataFrame to Parquet: {file_path}")
    
    def load_from_parquet(self, file_path: str) -> pd.DataFrame:
        """Load DataFrame from Parquet format."""
        df = pd.read_parquet(file_path)
        self.logger.info(f"Loaded DataFrame from Parquet: {file_path}")
        return df
    
    def get_subset(self, df: pd.DataFrame, start_idx: int, end_idx: int) -> np.ndarray:
        """Get numpy array subset for efficient computation."""
        return df.iloc[start_idx:end_idx].values


class MemoryManager:
    """Manage memory usage during optimization."""
    
    def __init__(self, memory_threshold: float = 0.8):
        self.memory_threshold = memory_threshold
        self.logger = system_logger.getChild("MemoryManager")
        self.cleanup_counter = 0
    
    def check_memory_usage(self) -> bool:
        """Check and manage memory usage."""
        memory_percent = psutil.virtual_memory().percent / 100
        
        if memory_percent > self.memory_threshold:
            self.logger.warning(f"Memory usage high: {memory_percent:.1%}")
            self._cleanup_memory()
            return True
        return False
    
    def _cleanup_memory(self):
        """Clean up memory by forcing garbage collection."""
        self.cleanup_counter += 1
        self.logger.info(f"Performing memory cleanup #{self.cleanup_counter}")
        
        # Force garbage collection
        gc.collect()
        
        # Get memory usage after cleanup
        memory_after = psutil.virtual_memory().percent / 100
        self.logger.info(f"Memory usage after cleanup: {memory_after:.1%}")
    
    def profile_memory_usage(self) -> Dict[str, float]:
        """Profile current memory usage."""
        memory_info = psutil.virtual_memory()
        return {
            'total_gb': memory_info.total / (1024**3),
            'available_gb': memory_info.available / (1024**3),
            'used_gb': memory_info.used / (1024**3),
            'percentage': memory_info.percent
        }


class EnhancedTrainingManagerOptimized:
    """
    Enhanced training manager with comprehensive optimization strategies.
    
    Implements:
    1. Cached backtesting to avoid redundant calculations
    2. Progressive evaluation to stop unpromising trials early
    3. Parallel backtesting for multiple parameter combinations
    4. Incremental training to reuse model states
    5. Streaming for large datasets
    6. Adaptive sampling to focus on promising regions
    7. Memory-efficient data structures
    8. Memory profiling and leak detection
    """
    
    def __init__(self, config: Dict[str, Any]) -> None:
        """Initialize enhanced training manager with optimizations."""
        self.config = config
        self.logger = system_logger.getChild("EnhancedTrainingManagerOptimized")
        
        # Training state
        self.is_training = False
        self.training_results = {}
        self.training_history = []
        
        # Initialize optimization components
        self.cached_backtester: Optional[CachedBacktester] = None
        self.progressive_evaluator: Optional[ProgressiveEvaluator] = None
        self.parallel_backtester: Optional[ParallelBacktester] = None
        self.incremental_trainer: Optional[IncrementalTrainer] = None
        self.streaming_processor: Optional[StreamingDataProcessor] = None
        self.adaptive_sampler: Optional[AdaptiveSampler] = None
        self.memory_manager = MemoryManager()
        self.data_manager = MemoryEfficientDataManager()
        
        # Configuration
        self.optimization_config = self.config.get("computational_optimization", {})
        self._load_optimization_config()
        
        # Model storage
        self.analyst_models = {}
        self.tactician_models = {}
        self.ensemble_creator = None
        self.calibration_systems = {}
        
    def _load_optimization_config(self):
        """Load optimization configuration."""
        # Caching configuration
        caching_config = self.optimization_config.get("caching", {})
        self.enable_caching = caching_config.get("enabled", True)
        self.max_cache_size = caching_config.get("max_cache_size", 1000)
        self.cache_ttl = caching_config.get("cache_ttl", 3600)
        
        # Parallelization configuration
        parallel_config = self.optimization_config.get("parallelization", {})
        self.enable_parallelization = parallel_config.get("enabled", True)
        self.max_workers = parallel_config.get("max_workers", 8)
        self.chunk_size = parallel_config.get("chunk_size", 1000)
        
        # Early stopping configuration
        early_stop_config = self.optimization_config.get("early_stopping", {})
        self.enable_early_stopping = early_stop_config.get("enabled", True)
        self.patience = early_stop_config.get("patience", 10)
        self.min_trials = early_stop_config.get("min_trials", 20)
        
        # Memory management configuration
        memory_config = self.optimization_config.get("memory_management", {})
        self.enable_memory_management = memory_config.get("enabled", True)
        self.memory_threshold = memory_config.get("memory_threshold", 0.8)
        self.cleanup_frequency = memory_config.get("cleanup_frequency", 100)
        
        self.logger.info("Loaded optimization configuration")
    
    @handle_specific_errors(
        error_handlers={
            ValueError: (False, "Invalid configuration"),
            AttributeError: (False, "Missing required parameters"),
            KeyError: (False, "Missing configuration keys"),
        },
        default_return=False,
        context="initialization",
    )
    async def initialize(self) -> bool:
        """Initialize the enhanced training manager with optimizations."""
        try:
            self.logger.info("ðŸš€ Initializing Enhanced Training Manager with Optimizations...")
            
            # Initialize optimization components
            if self.enable_parallelization:
                self.parallel_backtester = ParallelBacktester(n_workers=self.max_workers)
                self.logger.info(f"âœ… Parallel backtester initialized with {self.max_workers} workers")
            
            self.streaming_processor = StreamingDataProcessor(chunk_size=self.chunk_size)
            self.adaptive_sampler = AdaptiveSampler()
            
            # Initialize incremental trainer with base config
            base_model_config = self.config.get("model", {})
            self.incremental_trainer = IncrementalTrainer(base_model_config)
            
            self.logger.info("âœ… All optimization components initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Initialization failed: {e}")
            return False
    
    async def execute_optimized_training(self, symbol: str, exchange: str, 
                                       timeframe: str = "1h") -> Dict[str, Any]:
        """Execute training with all optimizations enabled."""
        try:
            self.is_training = True
            self.logger.info(f"ðŸŽ¯ Starting optimized training for {symbol} on {exchange}")
            
            # Step 1: Load and optimize data
            market_data = await self._load_and_optimize_data(symbol, exchange, timeframe)
            
            # Step 2: Initialize cached backtester if enabled
            if self.enable_caching:
                self.cached_backtester = CachedBacktester(market_data)
                self.logger.info("âœ… Cached backtester initialized")
            
            # Step 3: Initialize progressive evaluator if enabled
            if self.enable_early_stopping:
                self.progressive_evaluator = ProgressiveEvaluator(market_data)
                self.logger.info("âœ… Progressive evaluator initialized")
            
            # Step 4: Execute optimized training pipeline
            training_results = await self._execute_training_pipeline(
                market_data, symbol, exchange, timeframe
            )
            
            # Step 5: Memory cleanup
            if self.enable_memory_management:
                self.memory_manager.check_memory_usage()
            
            self.training_results = training_results
            return training_results
            
        except Exception as e:
            self.logger.error(f"âŒ Optimized training failed: {e}")
            return {}
        finally:
            self.is_training = False
    
    async def _load_and_optimize_data(self, symbol: str, exchange: str, 
                                    timeframe: str) -> pd.DataFrame:
        """Load and optimize data for training."""
        # Try to load from cache first
        cache_key = f"{symbol}_{exchange}_{timeframe}"
        
        # Check for Parquet files first (more efficient)
        parquet_path = f"data_cache/{cache_key}.parquet"
        if os.path.exists(parquet_path):
            self.logger.info(f"Loading data from Parquet: {parquet_path}")
            data = self.data_manager.load_from_parquet(parquet_path)
        else:
            # Fallback to CSV or other formats
            csv_path = f"data_cache/klines_{exchange}_{symbol}_{timeframe}_*.csv"
            csv_files = list(Path().glob(csv_path))
            
            if csv_files:
                self.logger.info(f"Loading and streaming data from {len(csv_files)} CSV files")
                # Use streaming processor for large datasets
                data_chunks = []
                for csv_file in csv_files:
                    chunk_data = self.streaming_processor.process_data_stream(str(csv_file))
                    data_chunks.append(chunk_data)
                data = pd.concat(data_chunks, ignore_index=True)
                
                # Save optimized version as Parquet for future use
                optimized_data = self.data_manager.optimize_dataframe(data)
                self.data_manager.save_to_parquet(optimized_data, parquet_path)
                data = optimized_data
            else:
                raise FileNotFoundError(f"No data found for {symbol} on {exchange}")
        
        # Optimize DataFrame memory usage
        data = self.data_manager.optimize_dataframe(data)
        
        self.logger.info(f"âœ… Data loaded and optimized: {len(data)} rows")
        return data
    
    async def _execute_training_pipeline(self, market_data: pd.DataFrame, 
                                       symbol: str, exchange: str, 
                                       timeframe: str) -> Dict[str, Any]:
        """Execute the full training pipeline with optimizations."""
        results = {}
        
        try:
            # Step 1: Data Collection (optimized)
            self.logger.info("ðŸ“Š Step 1: Optimized Data Collection")
            data_collection_results = await self._optimized_data_collection(
                market_data, symbol, exchange, timeframe
            )
            results['data_collection'] = data_collection_results
            
            # Memory check
            if self.enable_memory_management:
                self.memory_manager.check_memory_usage()
            
            # Step 2: Market Regime Classification (with caching)
            self.logger.info("ðŸ›ï¸ Step 2: Market Regime Classification")
            regime_results = await self._optimized_regime_classification(market_data)
            results['regime_classification'] = regime_results
            
            # Step 3: Progressive Hyperparameter Optimization
            self.logger.info("ðŸ”§ Step 3: Progressive Hyperparameter Optimization")
            optimization_results = await self._progressive_hyperparameter_optimization(
                market_data, symbol, exchange, timeframe
            )
            results['hyperparameter_optimization'] = optimization_results
            
            # Step 4: Incremental Model Training
            self.logger.info("ðŸ¤– Step 4: Incremental Model Training")
            model_results = await self._incremental_model_training(
                market_data, optimization_results
            )
            results['model_training'] = model_results
            
            # Step 5: Parallel Ensemble Creation
            if self.enable_parallelization:
                self.logger.info("ðŸŽ¼ Step 5: Parallel Ensemble Creation")
                ensemble_results = await self._parallel_ensemble_creation(model_results)
                results['ensemble_creation'] = ensemble_results
            
            self.logger.info("âœ… Optimized training pipeline completed successfully")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Training pipeline failed: {e}")
            return results
    
    async def _optimized_data_collection(self, market_data: pd.DataFrame, 
                                       symbol: str, exchange: str, 
                                       timeframe: str) -> Dict[str, Any]:
        """Optimized data collection with caching and streaming."""
        # Use the already optimized market data
        return {
            'status': 'success',
            'rows': len(market_data),
            'memory_usage_mb': market_data.memory_usage(deep=True).sum() / 1024**2,
            'data_types': dict(market_data.dtypes)
        }
    
    async def _optimized_regime_classification(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """Optimized regime classification with caching."""
        # Implement regime classification with caching
        # This is a placeholder - implement your actual regime classification logic
        return {
            'status': 'success',
            'regimes_identified': ['bull', 'bear', 'sideways'],
            'classification_accuracy': 0.85
        }
    
    async def _progressive_hyperparameter_optimization(self, market_data: pd.DataFrame,
                                                     symbol: str, exchange: str,
                                                     timeframe: str) -> Dict[str, Any]:
        """Progressive hyperparameter optimization with adaptive sampling."""
        if not self.adaptive_sampler:
            self.adaptive_sampler = AdaptiveSampler()
        
        # Define parameter bounds
        parameter_bounds = {
            'learning_rate': (0.01, 0.3),
            'max_depth': (3, 10),
            'n_estimators': (50, 500),
            'subsample': (0.6, 1.0),
            'colsample_bytree': (0.6, 1.0)
        }
        
        best_score = -np.inf
        best_params = None
        n_trials = self.config.get('n_trials', 100)
        
        for trial in range(n_trials):
            # Suggest parameters using adaptive sampling
            params = self.adaptive_sampler.suggest_parameters(parameter_bounds)
            
            # Progressive evaluation if enabled
            if self.enable_early_stopping and self.progressive_evaluator:
                score = self.progressive_evaluator.evaluate_progressively(
                    params, self._evaluate_params
                )
            elif self.enable_caching and self.cached_backtester:
                score = self.cached_backtester.run_cached_backtest(params)
            else:
                score = self._evaluate_params(market_data, params)
            
            # Update adaptive sampler
            self.adaptive_sampler.update_trial_history(params, score)
            
            if score > best_score:
                best_score = score
                best_params = params
            
            # Memory management
            if self.enable_memory_management and trial % self.cleanup_frequency == 0:
                self.memory_manager.check_memory_usage()
            
            self.logger.info(f"Trial {trial+1}/{n_trials}: Score = {score:.4f}")
        
        return {
            'status': 'success',
            'best_score': best_score,
            'best_params': best_params,
            'n_trials_completed': n_trials
        }
    
    def _evaluate_params(self, market_data: pd.DataFrame, params: Dict[str, Any]) -> float:
        """Evaluate parameter set (placeholder)."""
        # This is a placeholder - implement your actual evaluation logic
        return random.uniform(-1.0, 1.0)
    
    async def _incremental_model_training(self, market_data: pd.DataFrame,
                                        optimization_results: Dict[str, Any]) -> Dict[str, Any]:
        """Incremental model training to reuse model states."""
        if not self.incremental_trainer:
            base_config = self.config.get('model', {})
            self.incremental_trainer = IncrementalTrainer(base_config)
        
        best_params = optimization_results.get('best_params', {})
        
        # Prepare training data (placeholder)
        X = market_data[['open', 'high', 'low', 'close', 'volume']].values
        y = (market_data['close'].shift(-1) > market_data['close']).astype(int).values[:-1]
        X = X[:-1]  # Align with y
        
        # Incremental training
        model = self.incremental_trainer.train_incrementally(best_params, X, y)
        
        return {
            'status': 'success',
            'model_trained': model is not None,
            'training_samples': len(X),
            'features': X.shape[1] if len(X) > 0 else 0
        }
    
    async def _parallel_ensemble_creation(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """Parallel ensemble creation."""
        if not self.parallel_backtester:
            self.parallel_backtester = ParallelBacktester(n_workers=self.max_workers)
        
        # Create ensemble parameters for parallel evaluation
        ensemble_params = [
            {'model_type': 'xgb', 'weight': 0.4},
            {'model_type': 'lgb', 'weight': 0.3},
            {'model_type': 'cat', 'weight': 0.3}
        ]
        
        # Placeholder market data for parallel evaluation
        dummy_data = pd.DataFrame({
            'close': np.random.randn(1000),
            'volume': np.random.randn(1000)
        })
        
        # Parallel evaluation
        ensemble_scores = self.parallel_backtester.evaluate_batch(
            ensemble_params, dummy_data
        )
        
        return {
            'status': 'success',
            'ensemble_models': len(ensemble_params),
            'ensemble_scores': ensemble_scores,
            'parallel_workers': self.max_workers
        }
    
    def get_memory_profile(self) -> Dict[str, Any]:
        """Get current memory profile."""
        return self.memory_manager.profile_memory_usage()
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """Get optimization statistics."""
        stats = {
            'caching_enabled': self.enable_caching,
            'parallelization_enabled': self.enable_parallelization,
            'early_stopping_enabled': self.enable_early_stopping,
            'memory_management_enabled': self.enable_memory_management,
            'max_workers': self.max_workers,
            'memory_threshold': self.memory_threshold
        }
        
        if self.cached_backtester:
            stats['cache_size'] = len(self.cached_backtester.cache)
        
        if self.adaptive_sampler:
            stats['trial_history_size'] = len(self.adaptive_sampler.trial_history)
        
        return stats
    
    async def cleanup(self):
        """Clean up resources."""
        self.logger.info("ðŸ§¹ Cleaning up resources...")
        
        # Clean up parallel backtester
        if self.parallel_backtester:
            del self.parallel_backtester
        
        # Force garbage collection
        if self.enable_memory_management:
            self.memory_manager._cleanup_memory()
        
        self.logger.info("âœ… Cleanup completed")