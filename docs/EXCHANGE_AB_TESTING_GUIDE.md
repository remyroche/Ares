# Exchange A/B Testing Guide

## üéØ **Overview**

This guide explains how to use the Exchange A/B Testing Framework to simultaneously test the same model across different exchanges and compare performance.

## üöÄ **Key Features**

- **Simultaneous Testing**: Run the same model on multiple exchanges at once
- **Performance Comparison**: Compare P&L, accuracy, slippage, and execution rates
- **Volume Adaptation**: Automatic position size adjustments for different exchanges
- **Real-time Monitoring**: Track performance metrics in real-time
- **Statistical Analysis**: Identify the best performing exchange
- **Result Storage**: Save detailed results for analysis

## üìä **What Gets Compared**

### **Key Metrics Tracked:**
1. **Total Predictions**: Number of model predictions made
2. **Total Executions**: Number of trades actually executed
3. **Execution Rate**: Percentage of predictions that become trades
4. **Total P&L**: Cumulative profit/loss across all trades
5. **Accuracy**: Percentage of profitable trades
6. **Average Slippage**: Average slippage cost per trade
7. **Position Size Adjustments**: How position sizes are adapted per exchange

## üîß **Quick Start Example**

```python
from src.supervisor.exchange_ab_tester import setup_exchange_ab_tester, ABTestConfig

# Initialize A/B tester
config = {
    "exchange_ab_tester": {
        "result_storage_path": "ab_test_results"
    }
}

ab_tester = await setup_exchange_ab_tester(config)

# Configure A/B test
test_config = ABTestConfig(
    test_name="ETH_model_comparison",
    model_id="eth_lstm_v1",
    exchanges=["BINANCE", "MEXC", "GATEIO"],
    test_duration_hours=24,
    min_confidence_threshold=0.6,
    max_position_size=0.05
)

# Start the test
await ab_tester.start_ab_test(test_config)

# Process predictions for each exchange
for exchange in test_config.exchanges:
    # Get model prediction (this would come from your model)
    prediction = model.predict(features)
    confidence = model.confidence_score
    
    # Market data for the exchange
    market_data = {
        "price": 3500.0,
        "volume": 1000000,
        "spread": 0.001
    }
    
    # Process the prediction
    result = await ab_tester.process_prediction(
        exchange=exchange,
        prediction=prediction,
        confidence=confidence,
        market_data=market_data
    )

# Stop the test and get results
await ab_tester.stop_ab_test()
```

## üìà **Expected Results**

### **Sample Output:**
```
üöÄ Started A/B test 'ETH_model_comparison' across 3 exchanges

üìä BINANCE: prediction=0.0234, confidence=0.850, executed=True
üìä MEXC: prediction=0.0234, confidence=0.680, executed=True  
üìä GATEIO: prediction=0.0234, confidence=0.650, executed=True

üìä BINANCE: prediction=-0.0156, confidence=0.720, executed=True
üìä MEXC: prediction=-0.0156, confidence=0.580, executed=False
üìä GATEIO: prediction=-0.0156, confidence=0.550, executed=False

üõë Stopping A/B test...

üìä Generating A/B test results...

üèÜ Exchange Performance Summary:
  BINANCE: P&L=0.0012, Accuracy=0.750, ExecRate=1.000
  MEXC: P&L=0.0008, Accuracy=0.600, ExecRate=0.500
  GATEIO: P&L=0.0006, Accuracy=0.550, ExecRate=0.400

ü•á Best P&L: BINANCE (0.0012)
üéØ Best Accuracy: BINANCE (0.750)
```

## üéØ **Configuration Options**

### **ABTestConfig Parameters:**

```python
ABTestConfig(
    test_name="your_test_name",           # Unique test identifier
    model_id="your_model_id",             # Model being tested
    exchanges=["BINANCE", "MEXC"],        # Exchanges to test
    test_duration_hours=24,               # How long to run the test
    sample_interval_seconds=60,           # How often to sample
    min_confidence_threshold=0.6,         # Minimum confidence to execute
    max_position_size=0.05                # Maximum position size (5%)
)
```

### **Exchange-Specific Adjustments:**

The framework automatically applies exchange-specific adjustments:

- **Binance**: 100% position size (baseline)
- **MEXC**: 40% position size (60% reduction)
- **Gate.io**: 40% position size (60% reduction)

## üìä **Integration with Your Pipeline**

### **1. Initialize in Your Trading System:**

```python
# In your main trading pipeline
class TradingSystem:
    def __init__(self, config):
        self.ab_tester = None
        self.config = config
    
    async def initialize(self):
        # Initialize A/B tester
        self.ab_tester = await setup_exchange_ab_tester(self.config)
        
        # Start A/B test if configured
        if self.config.get("enable_ab_testing", False):
            test_config = ABTestConfig(
                test_name="live_model_test",
                model_id=self.model_id,
                exchanges=self.config.get("ab_test_exchanges", ["BINANCE", "MEXC"]),
                test_duration_hours=24
            )
            await self.ab_tester.start_ab_test(test_config)
```

### **2. Process Predictions:**

```python
async def process_model_prediction(self, exchange: str, features: np.ndarray):
    # Get model prediction
    prediction = self.model.predict(features)
    confidence = self.model.confidence_score
    
    # Get market data
    market_data = await self.get_market_data(exchange)
    
    # Process through A/B tester
    if self.ab_tester and self.ab_tester.is_running:
        result = await self.ab_tester.process_prediction(
            exchange=exchange,
            prediction=prediction,
            confidence=confidence,
            market_data=market_data
        )
        
        # Log the result
        self.logger.info(f"A/B Test Result: {result}")
    
    # Continue with normal trading logic
    return prediction, confidence
```

### **3. Monitor Results:**

```python
async def monitor_ab_test(self):
    while True:
        if self.ab_tester and self.ab_tester.is_running:
            status = self.ab_tester.get_test_status()
            
            self.logger.info(f"A/B Test Status: {status}")
            
            # Check if test should end
            if status.get("is_running", False):
                # Continue monitoring
                await asyncio.sleep(60)
            else:
                # Test completed
                break
        else:
            await asyncio.sleep(60)
```

## üîç **Understanding Results**

### **Key Insights to Look For:**

1. **Execution Rate Differences**:
   - Higher confidence thresholds on smaller exchanges
   - Volume constraints preventing trades

2. **P&L Performance**:
   - Which exchange generates the most profit
   - Impact of slippage and spread costs

3. **Accuracy Variations**:
   - Model performance differences across exchanges
   - Data quality impact on predictions

4. **Slippage Analysis**:
   - Execution quality differences
   - Market impact variations

### **Sample Analysis:**

```python
# Load saved results
import json

with open("ab_test_results/ab_test_ETH_model_comparison_20250103_140000.json", "r") as f:
    results = json.load(f)

# Analyze performance
for exchange, metrics in results["performance_metrics"].items():
    print(f"\n{exchange} Analysis:")
    print(f"  Total Predictions: {metrics['total_predictions']}")
    print(f"  Execution Rate: {metrics['total_executions']/metrics['total_predictions']:.2%}")
    print(f"  Total P&L: {metrics['total_profit_loss']:.4f}")
    print(f"  Accuracy: {metrics['accuracy']:.2%}")
    print(f"  Avg Slippage: {metrics['avg_slippage']:.4f}")
```

## üéØ **Best Practices**

### **1. Test Duration**
- **Short tests** (1-4 hours): Quick validation
- **Medium tests** (24-48 hours): Standard comparison
- **Long tests** (1-7 days): Comprehensive analysis

### **2. Sample Frequency**
- **High frequency** (30-60 seconds): Detailed analysis
- **Medium frequency** (5-15 minutes): Standard monitoring
- **Low frequency** (1 hour): Long-term trends

### **3. Confidence Thresholds**
- **Conservative** (0.7-0.8): Fewer trades, higher quality
- **Moderate** (0.6-0.7): Balanced approach
- **Aggressive** (0.5-0.6): More trades, lower quality

### **4. Position Sizing**
- **Small positions** (1-2%): Conservative testing
- **Medium positions** (3-5%): Standard testing
- **Large positions** (5-10%): Aggressive testing

## üö® **Important Considerations**

### **1. Volume Constraints**
- Smaller exchanges may have limited liquidity
- Position sizes are automatically reduced
- Monitor execution rates for volume issues

### **2. Data Quality**
- Different exchanges have different data quality
- Model confidence is adjusted accordingly
- Monitor accuracy differences

### **3. Market Impact**
- Larger positions on smaller exchanges can move markets
- Framework simulates market impact
- Consider reducing position sizes further if needed

### **4. Regulatory Differences**
- Different exchanges have different trading hours
- Some exchanges may have restrictions
- Monitor for regulatory-related issues

## üìà **Advanced Usage**

### **Custom Volume Adaptations:**

```python
# You can integrate with the ExchangeVolumeAdapter for more sophisticated adjustments
from src.supervisor.exchange_volume_adapter import setup_exchange_volume_adapter

volume_adapter = await setup_exchange_volume_adapter(config)

# Use volume adapter in your A/B test processing
adjusted_position_size = volume_adapter.calculate_position_size_adjustment(
    exchange=exchange,
    base_position_size=position_size,
    current_volume=market_data["volume"],
    confidence_score=confidence
)
```

### **Statistical Significance Testing:**

```python
# Add statistical analysis to your results
import scipy.stats as stats

def compare_exchanges_statistically(results):
    exchanges = list(results["performance_metrics"].keys())
    
    for i, exchange1 in enumerate(exchanges):
        for exchange2 in exchanges[i+1:]:
            # Compare P&L distributions
            pnl1 = [r["profit_loss"] for r in results["results"][exchange1] if r["profit_loss"]]
            pnl2 = [r["profit_loss"] for r in results["results"][exchange2] if r["profit_loss"]]
            
            if pnl1 and pnl2:
                t_stat, p_value = stats.ttest_ind(pnl1, pnl2)
                print(f"{exchange1} vs {exchange2}: p-value = {p_value:.4f}")
```

## üéØ **Conclusion**

The Exchange A/B Testing Framework provides a powerful way to validate model transfer learning across exchanges. By simultaneously testing the same model on different exchanges, you can:

1. **Identify the best performing exchange** for your model
2. **Validate transfer learning assumptions** about cross-exchange performance
3. **Optimize position sizing** for each exchange
4. **Monitor execution quality** and adjust strategies accordingly

Start with conservative settings and gradually increase complexity as you validate the framework's effectiveness for your specific use case. 