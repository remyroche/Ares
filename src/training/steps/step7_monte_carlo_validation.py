import asyncio
import json
import os
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

# Add the project root to the Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backtesting.ares_deep_analyzer import (
    run_monte_carlo_simulation,
)
from src.config import CONFIG  # Import CONFIG
from src.utils.error_handler import (
    handle_errors,
    handle_file_operations,
    handle_specific_errors,
)
from src.utils.logger import setup_logging, system_logger


class MonteCarloValidationStep:
    """
    Enhanced Monte Carlo validation step with comprehensive error handling and type safety.
    """

    def __init__(self, config: dict[str, Any]) -> None:
        """
        Initialize Monte Carlo validation step with enhanced type safety.

        Args:
            config: Configuration dictionary
        """
        self.config: dict[str, Any] = config
        self.logger = system_logger.getChild("MonteCarloValidationStep")

        # Validation state
        self.is_validating: bool = False
        self.validation_progress: float = 0.0
        self.last_validation_time: datetime | None = None
        self.validation_history: list[dict[str, Any]] = []

        # Configuration
        self.step_config: dict[str, Any] = self.config.get(
            "step7_monte_carlo_validation",
            {},
        )
        self.data_dir: str = self.step_config.get("data_directory", "data")
        self.models_dir: str = self.step_config.get("models_directory", "models")
        self.results_dir: str = self.step_config.get("results_directory", "results")
        self.n_simulations: int = self.step_config.get("n_simulations", 1000)

    @handle_specific_errors(
        error_handlers={
            ValueError: (False, "Invalid Monte Carlo validation configuration"),
            AttributeError: (False, "Missing required validation parameters"),
            KeyError: (False, "Missing configuration keys"),
        },
        default_return=False,
        context="Monte Carlo validation initialization",
    )
    async def initialize(self) -> bool:
        """
        Initialize Monte Carlo validation step with enhanced error handling.

        Returns:
            bool: True if initialization successful, False otherwise
        """
        try:
            self.logger.info("Initializing Monte Carlo Validation Step...")

            # Load validation configuration
            await self._load_validation_configuration()

            # Validate configuration
            if not self._validate_configuration():
                self.logger.error("Invalid configuration for Monte Carlo validation")
                return False

            # Initialize directories
            await self._initialize_directories()

            self.logger.info(
                "âœ… Monte Carlo Validation Step initialization completed successfully",
            )
            return True

        except Exception as e:
            self.logger.error(
                f"âŒ Monte Carlo Validation Step initialization failed: {e}",
            )
            return False

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="validation configuration loading",
    )
    async def _load_validation_configuration(self) -> None:
        """Load validation configuration."""
        try:
            # Set default validation parameters
            self.step_config.setdefault("data_directory", "data")
            self.step_config.setdefault("models_directory", "models")
            self.step_config.setdefault("results_directory", "results")
            self.step_config.setdefault("n_simulations", 1000)
            self.step_config.setdefault("confidence_level", 0.95)
            self.step_config.setdefault("random_seed", 42)
            self.step_config.setdefault("max_iterations", 10000)

            # Update configuration
            self.data_dir = self.step_config["data_directory"]
            self.models_dir = self.step_config["models_directory"]
            self.results_dir = self.step_config["results_directory"]
            self.n_simulations = self.step_config["n_simulations"]

            self.logger.info("Validation configuration loaded successfully")

        except Exception as e:
            self.logger.error(f"Error loading validation configuration: {e}")

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=False,
        context="configuration validation",
    )
    def _validate_configuration(self) -> bool:
        """
        Validate validation configuration.

        Returns:
            bool: True if configuration is valid, False otherwise
        """
        try:
            # Validate data directory
            if not self.data_dir or not os.path.exists(self.data_dir):
                self.logger.error("Invalid data directory")
                return False

            # Validate models directory
            if not self.models_dir:
                self.logger.error("Invalid models directory")
                return False

            # Validate results directory
            if not self.results_dir:
                self.logger.error("Invalid results directory")
                return False

            # Validate validation parameters
            if self.n_simulations <= 0:
                self.logger.error("Invalid number of simulations")
                return False

            self.logger.info("Configuration validation successful")
            return True

        except Exception as e:
            self.logger.error(f"Error validating configuration: {e}")
            return False

    @handle_file_operations(
        default_return=None,
        context="directory initialization",
    )
    async def _initialize_directories(self) -> None:
        """Initialize directories."""
        try:
            # Create results directory
            if not os.path.exists(self.results_dir):
                os.makedirs(self.results_dir, exist_ok=True)
                self.logger.info(f"Created results directory: {self.results_dir}")

            # Create subdirectories
            subdirs = ["monte_carlo_validation", "intermediate_results", "logs"]
            for subdir in subdirs:
                subdir_path = os.path.join(self.results_dir, subdir)
                if not os.path.exists(subdir_path):
                    os.makedirs(subdir_path, exist_ok=True)
                    self.logger.info(f"Created subdirectory: {subdir_path}")

            self.logger.info("Directories initialized successfully")

        except Exception as e:
            self.logger.error(f"Error initializing directories: {e}")

    @handle_specific_errors(
        error_handlers={
            ValueError: (False, "Invalid validation parameters"),
            AttributeError: (False, "Missing validation components"),
            KeyError: (False, "Missing required data"),
        },
        default_return=False,
        context="Monte Carlo validation execution",
    )
    async def execute(self) -> bool:
        """
        Execute Monte Carlo validation with enhanced error handling.

        Returns:
            bool: True if validation successful, False otherwise
        """
        try:
            self.logger.info("Starting Monte Carlo validation...")

            self.is_validating = True
            self.validation_progress = 0.0

            # Load training data
            training_data = await self._load_training_data()
            if training_data is None:
                self.logger.error("Failed to load training data")
                return False

            # Load trained model
            trained_model = await self._load_trained_model()
            if trained_model is None:
                self.logger.error("Failed to load trained model")
                return False

            # Perform Monte Carlo validation
            validation_result = await self._perform_monte_carlo_validation(
                training_data,
                trained_model,
            )
            if validation_result is None:
                self.logger.error("Failed to perform Monte Carlo validation")
                return False

            # Save validation results
            await self._save_validation_results(validation_result)

            # Update validation state
            self.is_validating = False
            self.last_validation_time = datetime.now()

            # Record validation history
            self.validation_history.append(
                {
                    "timestamp": self.last_validation_time,
                    "n_simulations": self.n_simulations,
                    "result": validation_result,
                },
            )

            self.logger.info("âœ… Monte Carlo validation completed successfully")
            return True

        except Exception as e:
            self.logger.error(f"âŒ Monte Carlo validation failed: {e}")
            self.is_validating = False
            return False

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="training data loading",
    )
    async def _load_training_data(self) -> pd.DataFrame | None:
        """
        Load training data for validation.

        Returns:
            Optional[pd.DataFrame]: Training data or None if failed
        """
        try:
            # Look for consolidated data files
            data_files = []
            for file in os.listdir(self.data_dir):
                if file.endswith("_consolidated.csv"):
                    data_files.append(os.path.join(self.data_dir, file))

            if not data_files:
                self.logger.error("No consolidated data files found")
                return None

            # Load the first available data file
            data_file = data_files[0]
            self.logger.info(f"Loading training data from: {data_file}")

            # Load data
            data = pd.read_csv(data_file)

            # Basic data validation
            if data.empty:
                self.logger.error("Training data is empty")
                return None

            required_columns = ["timestamp", "open", "high", "low", "close", "volume"]
            missing_columns = [
                col for col in required_columns if col not in data.columns
            ]

            if missing_columns:
                self.logger.error(f"Missing required columns: {missing_columns}")
                return None

            self.logger.info(f"Loaded training data with {len(data)} rows")
            return data

        except Exception as e:
            self.logger.error(f"Error loading training data: {e}")
            return None

    @handle_file_operations(
        default_return=None,
        context="trained model loading",
    )
    async def _load_trained_model(self) -> dict[str, Any] | None:
        """
        Load trained model for validation.

        Returns:
            Optional[Dict[str, Any]]: Trained model or None if not found
        """
        try:
            # Look for trained model files
            model_files = []
            for file in os.listdir(self.models_dir):
                if file.endswith(".joblib") or file.endswith(".pkl"):
                    model_files.append(os.path.join(self.models_dir, file))

            if not model_files:
                self.logger.warning("No trained model files found")
                return None

            # Load the first available model file
            model_file = model_files[0]
            self.logger.info(f"Loading trained model from: {model_file}")

            # Simulate model loading
            model_info = {
                "model_path": model_file,
                "model_type": "neural_network",
                "model_size": os.path.getsize(model_file),
                "load_time": datetime.now(),
            }

            return model_info

        except Exception as e:
            self.logger.error(f"Error loading trained model: {e}")
            return None

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="Monte Carlo validation execution",
    )
    async def _perform_monte_carlo_validation(
        self,
        data: pd.DataFrame,
        trained_model: dict[str, Any],
    ) -> dict[str, Any] | None:
        """
        Perform Monte Carlo validation.

        Args:
            data: Training data DataFrame
            trained_model: Trained model information

        Returns:
            Optional[Dict[str, Any]]: Validation results or None if failed
        """
        try:
            # Simulate validation process
            await asyncio.sleep(4)  # Simulate processing time

            # Generate sample validation results
            validation_result = {
                "n_simulations": self.n_simulations,
                "total_samples": len(data),
                "simulations": [],
                "statistics": {
                    "mean_return": np.random.uniform(0.05, 0.25),
                    "std_return": np.random.uniform(0.10, 0.30),
                    "sharpe_ratio": np.random.uniform(0.5, 2.5),
                    "max_drawdown": np.random.uniform(0.05, 0.25),
                    "var_95": np.random.uniform(0.02, 0.08),
                    "cvar_95": np.random.uniform(0.03, 0.12),
                },
                "confidence_intervals": {
                    "return_ci": [
                        np.random.uniform(0.03, 0.20),
                        np.random.uniform(0.25, 0.35),
                    ],
                    "sharpe_ci": [
                        np.random.uniform(0.3, 1.5),
                        np.random.uniform(1.8, 3.0),
                    ],
                    "drawdown_ci": [
                        np.random.uniform(0.03, 0.15),
                        np.random.uniform(0.20, 0.35),
                    ],
                },
                "validation_time": np.random.uniform(60, 300),
            }

            # Generate results for each simulation
            for i in range(min(10, self.n_simulations)):  # Limit for demo
                simulation_result = {
                    "simulation": i + 1,
                    "total_return": np.random.uniform(-0.1, 0.5),
                    "sharpe_ratio": np.random.uniform(0.2, 3.0),
                    "max_drawdown": np.random.uniform(0.02, 0.4),
                    "win_rate": np.random.uniform(0.4, 0.8),
                    "profit_factor": np.random.uniform(0.7, 3.0),
                }
                validation_result["simulations"].append(simulation_result)

            self.logger.info(
                f"Monte Carlo validation completed with mean return: {validation_result['statistics']['mean_return']:.4f}",
            )
            return validation_result

        except Exception as e:
            self.logger.error(f"Error performing Monte Carlo validation: {e}")
            return None

    @handle_file_operations(
        default_return=None,
        context="validation results saving",
    )
    async def _save_validation_results(self, results: dict[str, Any]) -> None:
        """
        Save validation results to file.

        Args:
            results: Validation results dictionary
        """
        try:
            # Create results filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = os.path.join(
                self.results_dir,
                "monte_carlo_validation",
                f"monte_carlo_validation_{timestamp}.json",
            )

            # Save results
            import json

            with open(results_file, "w") as f:
                json.dump(results, f, indent=2, default=str)

            self.logger.info(f"Validation results saved to: {results_file}")

        except Exception as e:
            self.logger.error(f"Error saving validation results: {e}")

    def get_validation_status(self) -> dict[str, Any]:
        """
        Get validation status information.

        Returns:
            Dict[str, Any]: Validation status
        """
        return {
            "is_validating": self.is_validating,
            "validation_progress": self.validation_progress,
            "last_validation_time": self.last_validation_time,
            "n_simulations": self.n_simulations,
            "data_directory": self.data_dir,
            "models_directory": self.models_dir,
            "results_directory": self.results_dir,
            "validation_history_count": len(self.validation_history),
        }

    @handle_errors(
        exceptions=(Exception,),
        default_return=None,
        context="Monte Carlo validation cleanup",
    )
    async def stop(self) -> None:
        """Stop the Monte Carlo validation step."""
        self.logger.info("ðŸ›‘ Stopping Monte Carlo Validation Step...")

        try:
            # Stop validation if running
            if self.is_validating:
                self.is_validating = False
                self.logger.info("Monte Carlo validation stopped")

            self.logger.info("âœ… Monte Carlo Validation Step stopped successfully")

        except Exception as e:
            self.logger.error(f"Error stopping Monte Carlo validation: {e}")


@handle_errors(
    exceptions=(Exception,),
    default_return=False,
    context="monte_carlo_validation_step",
)
async def run_step(symbol: str, data_dir: str, timeframe: str = "1m", exchange: str = "BINANCE") -> bool:
    """
    Runs Monte Carlo validation using the fully prepared data from Step 4.
    Loads prepared data from the pickle file saved by step4.
    """
    setup_logging()
    logger = system_logger.getChild("Step7MonteCarloValidation")

    start_time = time.time()
    logger.info("=" * 80)
    logger.info("ðŸš€ STEP 7: MONTE CARLO VALIDATION")
    logger.info("=" * 80)
    logger.info(f"ðŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ðŸŽ¯ Symbol: {symbol}")
    logger.info(f"ðŸ“ Data directory: {data_dir}")

    # Check if this is a blank training run and override configuration accordingly
    blank_training_mode = os.environ.get("BLANK_TRAINING_MODE", "0") == "1"
    if blank_training_mode:
        logger.info(
            "ðŸ”§ BLANK TRAINING MODE DETECTED - Overriding configuration for faster execution",
        )
        # Reduce Monte Carlo simulation parameters for blank training mode
        n_simulations = 100  # Reduced from default 1000
        simulation_days = 30  # Reduced simulation period
        logger.info(
            f"ðŸ”§ BLANK TRAINING MODE: Using reduced parameters (n_simulations={n_simulations}, simulation_days={simulation_days})",
        )
    else:
        n_simulations = 1000  # Default
        simulation_days = 90  # Default

    try:
        # Step 7.1: Load PREPARED data from the file saved in step 4
        logger.info("ðŸ“Š STEP 7.1: Loading prepared data from pickle file...")
        data_load_start = time.time()

        prepared_data_path = os.path.join(data_dir, f"{exchange}_{symbol}_prepared_data.pkl")
        if not os.path.exists(prepared_data_path):
            logger.warning(f"âš ï¸  Prepared data file not found: {prepared_data_path}")
            logger.info("ðŸ”§ Creating fallback prepared data from historical data...")

            # Load historical data as fallback
            historical_data_path = os.path.join(
                data_dir,
                f"{exchange}_{symbol}_historical_data.pkl",
            )
            if os.path.exists(historical_data_path):
                with open(historical_data_path, "rb") as f:
                    historical_data = pickle.load(f)

                # Use klines data as prepared data
                prepared_data = historical_data["klines"]

                # Add basic features for validation
                if not prepared_data.empty:
                    # Add basic technical indicators
                    prepared_data["sma_20"] = (
                        prepared_data["close"].rolling(window=20).mean()
                    )
                    prepared_data["rsi"] = 100 - (
                        100
                        / (
                            1
                            + (
                                prepared_data["close"].diff().rolling(window=14).mean()
                                / prepared_data["close"].diff().rolling(window=14).std()
                            )
                        )
                    )
                    prepared_data["volume_sma"] = (
                        prepared_data["volume"].rolling(window=20).mean()
                    )

                    # Drop NaN values
                    prepared_data = prepared_data.dropna()

                    logger.info(
                        f"âœ… Created fallback prepared data: {prepared_data.shape}",
                    )
                else:
                    logger.error("âŒ Historical data is empty")
                    return False
            else:
                logger.error(
                    f"âŒ Historical data file not found: {historical_data_path}",
                )
                return False
        else:
            with open(prepared_data_path, "rb") as f:
                prepared_data = pickle.load(f)

        data_load_duration = time.time() - data_load_start
        logger.info(f"â±ï¸  Data loading completed in {data_load_duration:.2f} seconds")
        logger.info(
            f"ðŸ“Š Loaded prepared data: {len(prepared_data)} rows, {len(prepared_data.columns)} columns",
        )

        if prepared_data.empty:
            logger.error(
                "âŒ Prepared data is empty for Monte Carlo validation. Aborting.",
            )
            return False

        # Step 7.2: Load best parameters
        logger.info("ðŸŽ¯ STEP 7.2: Loading best parameters...")
        params_load_start = time.time()

        best_params = CONFIG.get(
            "best_params",
            {},
        )  # Use the globally updated best_params

        params_load_duration = time.time() - params_load_start
        logger.info(
            f"â±ï¸  Parameters loading completed in {params_load_duration:.2f} seconds",
        )
        logger.info("âœ… Loaded best parameters:")
        for param, value in best_params.items():
            logger.info(f"   - {param}: {value}")

        # Step 7.3: Run Monte Carlo simulation
        logger.info("ðŸŽ² STEP 7.3: Running Monte Carlo simulation...")
        mc_start = time.time()

        logger.info("ðŸ”¢ Monte Carlo simulation parameters:")
        logger.info(f"   - Symbol: {symbol}")
        logger.info(f"   - Data shape: {prepared_data.shape}")
        logger.info(f"   - Best params count: {len(best_params)}")

        mc_curves, base_portfolio, mc_report_str = await run_monte_carlo_simulation(
            prepared_data,
            best_params,
        )

        mc_duration = time.time() - mc_start
        logger.info(f"â±ï¸  Monte Carlo simulation completed in {mc_duration:.2f} seconds")
        logger.info("ðŸ“Š Simulation results:")
        logger.info(
            f"   - Number of curves: {len(mc_curves) if mc_curves is not None else 0}",
        )
        logger.info(f"   - Base portfolio present: {base_portfolio is not None}")
        logger.info(f"   - Report length: {len(mc_report_str)} characters")

        # Step 7.4: Save reports
        logger.info("ðŸ’¾ STEP 7.4: Saving reports...")
        save_start = time.time()

        reports_dir = os.path.join(
            Path(__file__).parent.parent.parent,
            "reports",
        )  # Access reports dir
        os.makedirs(reports_dir, exist_ok=True)
        mc_file = os.path.join(reports_dir, f"{symbol}_monte_carlo_report.txt")

        with open(mc_file, "w") as f:
            f.write(mc_report_str)

        # Parse metrics from the report string
        metrics = _parse_report_for_metrics(mc_report_str)

        # Save metrics to a JSON file for the TrainingManager
        output_metrics_file = os.path.join(data_dir, f"{symbol}_mc_metrics.json")
        with open(output_metrics_file, "w") as f:
            json.dump(metrics, f, indent=4)

        save_duration = time.time() - save_start
        logger.info(f"â±ï¸  Reports saved in {save_duration:.2f} seconds")
        logger.info("ðŸ“„ Reports saved to:")
        logger.info(f"   - Monte Carlo report: {mc_file}")
        logger.info(f"   - Metrics JSON: {output_metrics_file}")
        logger.info("ðŸ“Š Extracted metrics:")
        for metric, value in metrics.items():
            logger.info(f"   - {metric}: {value}")

        # Final summary
        total_duration = time.time() - start_time
        logger.info("=" * 80)
        logger.info("ðŸŽ‰ STEP 7: MONTE CARLO VALIDATION COMPLETE")
        logger.info("=" * 80)
        logger.info(f"â±ï¸  Total duration: {total_duration:.2f} seconds")
        logger.info("ðŸ“„ Output files:")
        logger.info(f"   - Monte Carlo report: {mc_file}")
        logger.info(f"   - Metrics JSON: {output_metrics_file}")
        logger.info("âœ… Success: True")

        return True
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error("=" * 80)
        logger.error("âŒ STEP 7: MONTE CARLO VALIDATION FAILED")
        logger.error("=" * 80)
        logger.error(f"â±ï¸  Duration before failure: {total_duration:.2f} seconds")
        logger.error(f"ðŸ’¥ Error: {e}")
        logger.error("ðŸ“‹ Full traceback:", exc_info=True)
        return False


def _parse_report_for_metrics(report_content: str) -> dict[str, float]:
    """Parses a text report to extract key-value metrics."""
    import re

    metrics = {}
    # Adjusted patterns to match the output format of calculate_detailed_metrics and run_monte_carlo_simulation
    patterns = {
        "Original Final Equity": r"Original Final Equity:\s*\$([0-9,]+\.\d{2})",
        "Mean Simulated Equity": r"Mean Simulated Equity:\s*\$([0-9,]+\.\d{2})",
        "Confidence Interval Lower": r"Confidence Interval for Final Equity: \$([0-9,]+\.\d{2})",
        "Confidence Interval Upper": r"Confidence Interval for Final Equity: \$[0-9,]+\.\d{2} - \$([0-9,]+\.\d{2})",
    }
    for key, pattern in patterns.items():
        match = re.search(pattern, report_content)
        if match:
            try:
                # For ranges, we might need to extract both groups
                if "Confidence Interval" in key:
                    if "Lower" in key:
                        value = match.group(1).replace(",", "")
                    else:  # Upper
                        value = (
                            match.group(2).replace(",", "") if match.groups() else ""
                        )  # Ensure group exists
                else:
                    value = match.group(1).replace(",", "")
                metrics[key.replace(" ", "_").replace("%", "Pct").replace(".", "")] = (
                    float(value)
                )
            except (ValueError, IndexError):
                continue
    return metrics


if __name__ == "__main__":
    # Command-line arguments: symbol, data_dir
    symbol = sys.argv[1]
    data_dir = sys.argv[2]

    success = asyncio.run(run_step(symbol, data_dir))

    if not success:
        sys.exit(1)  # Indicate failure
    sys.exit(0)  # Indicate success
