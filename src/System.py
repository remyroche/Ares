"""
System module containing the AnalystLabelingFeatureEngineeringStep class.
"""

import os
import sys
import pickle
import pandas as pd
import numpy as np
from typing import Any, Dict, List
from pathlib import Path
from datetime import datetime, timedelta

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import system_logger
from src.analyst.unified_regime_classifier import UnifiedRegimeClassifier
from src.analyst.feature_engineering_orchestrator import FeatureEngineeringOrchestrator
from src.analyst.advanced_feature_engineering import AdvancedFeatureEngineering
from src.analyst.autoencoder_feature_generator import AutoencoderFeatureGenerator
from src.analyst.meta_labeling_system import MetaLabelingSystem
from src.analyst.data_utils import PrecomputedFeaturesManager
from src.analyst.data_utils import EfficientFeaturesDatabase
from src.config import CONFIG


class AnalystLabelingFeatureEngineeringStep:
    """Step 4: Analyst Labeling and Feature Engineering."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = system_logger.getChild("AnalystLabelingFeatureEngineeringStep")
        
        # Initialize components
        self.feature_engineering_orchestrator = None
        self.precomputed_features_manager = None
        self.efficient_features_database = None
        self.meta_labeling_system = None
        self.unified_regime_classifier = None
        
    async def initialize(self) -> None:
        """Initialize the analyst labeling and feature engineering step."""
        self.logger.info("Initializing Analyst Labeling and Feature Engineering Step...")
        
        # Initialize feature engineering orchestrator
        self.feature_engineering_orchestrator = FeatureEngineeringOrchestrator(self.config)
        
        # Initialize precomputed features manager
        self.precomputed_features_manager = PrecomputedFeaturesManager(self.config)
        
        # Initialize efficient features database
        self.efficient_features_database = EfficientFeaturesDatabase(self.config)
        
        # Initialize meta-labeling system
        self.meta_labeling_system = MetaLabelingSystem(self.config)
        
        # Initialize unified regime classifier
        self.unified_regime_classifier = UnifiedRegimeClassifier(self.config)
        
        self.logger.info("Analyst Labeling and Feature Engineering Step initialized successfully")
        
    async def execute(
        self,
        training_input: Dict[str, Any],
        pipeline_state: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Execute analyst labeling and feature engineering with incremental processing.
        
        Args:
            training_input: Training input parameters
            pipeline_state: Current pipeline state
            
        Returns:
            Dict: Updated pipeline state
        """
        self.logger.info("ðŸ”„ Executing Analyst Labeling and Feature Engineering with Incremental Processing...")
        
        # Extract parameters
        symbol = training_input.get("symbol", "ETHUSDT")
        exchange = training_input.get("exchange", "BINANCE")
        data_dir = training_input.get("data_dir", "data/training")
        
        # Load historical data
        data_file_path = f"{data_dir}/{exchange}_{symbol}_historical_data.pkl"
        
        if not os.path.exists(data_file_path):
            raise FileNotFoundError(f"Data file not found: {data_file_path}")
        
        with open(data_file_path, "rb") as f:
            historical_data = pickle.load(f)
        
        self.logger.info(f"Loaded {len(historical_data)} historical records for {symbol} on {exchange}")
        
        # Check for existing database
        if self.efficient_features_database.has_existing_database():
            self.logger.info("Found existing database, will process missing time ranges only")
        else:
            self.logger.info("No existing database found, will process all data")
        
        # Get missing time ranges
        missing_ranges = self.efficient_features_database.get_missing_time_ranges(historical_data)
        self.logger.info(f"Processing {len(missing_ranges)} missing time ranges")
        
        # Process each missing range
        for start_time, end_time in missing_ranges:
            self.logger.info(f"ðŸ”§ Processing NEW range with full feature engineering: {start_time} to {end_time}")
            
            # Filter data for this range
            # Convert timestamps to datetime if they're not already
            if historical_data['timestamp'].dtype == 'object':
                historical_data['timestamp'] = pd.to_datetime(historical_data['timestamp'])
            
            # Ensure start_time and end_time are datetime objects
            if isinstance(start_time, (int, float)):
                start_time = pd.to_datetime(start_time, unit='s')
            if isinstance(end_time, (int, float)):
                end_time = pd.to_datetime(end_time, unit='s')
            
            mask = (historical_data['timestamp'] >= start_time) & (historical_data['timestamp'] <= end_time)
            range_data = historical_data[mask]
            
            self.logger.info(f"Processing data range: {len(range_data)} records")
            
            # Generate comprehensive features
            self.logger.info(f"Generating comprehensive features for {symbol} on {exchange}")
            
            # Use feature engineering orchestrator
            features_df = await self.feature_engineering_orchestrator.generate_all_features(
                range_data, 
                symbol, 
                exchange
            )
            
            # Standardize feature names
            feature_names = list(features_df.columns)
            self.logger.info(f"Standardized {len(feature_names)} feature names")
            
            # Apply Tactician Triple Barrier Method
            self.logger.info(f"Applying Tactician Triple Barrier Method for {symbol} on {exchange}...")
            
            # Pre-calculate volatility
            self.logger.info("Pre-calculated volatility with a 100-period lookback.")
            
            # Apply triple barrier labeling
            labeled_data = await self._apply_triple_barrier_labeling(
                features_df, 
                symbol, 
                exchange
            )
            
            # Store features in database
            self.efficient_features_database.store_features(
                labeled_data, 
                start_time, 
                end_time, 
                symbol, 
                exchange
            )
        
        # Save final results
        results = {
            "features_generated": True,
            "labeling_completed": True,
            "database_updated": True,
            "duration": 0.0,
            "status": "SUCCESS"
        }
        
        # Update pipeline state
        pipeline_state["analyst_labeling_feature_engineering"] = results
        
        return results
    
    async def _apply_triple_barrier_labeling(
        self,
        data: pd.DataFrame,
        symbol: str,
        exchange: str
    ) -> pd.DataFrame:
        """
        Apply Triple Barrier Method for labeling.
        
        Args:
            data: Market data with features
            symbol: Trading symbol
            exchange: Exchange name
            
        Returns:
            DataFrame with labels added
        """
        try:
            # Ensure we have required columns
            required_columns = ["open", "high", "low", "close", "volume"]
            for col in required_columns:
                if col not in data.columns:
                    raise ValueError(f"Missing required column: {col}")
            
            # Create a copy for labeling
            labeled_data = data.copy()
            
            # Calculate volatility for barrier sizing
            returns = np.log(data['close'] / data['close'].shift(1))
            volatility = returns.rolling(window=100).std()
            
            # Define barrier parameters
            profit_take_multiplier = 2.5  # 0.25%
            stop_loss_multiplier = 1.5    # 0.15%
            time_barrier_periods = 100     # 100 periods
            
            # Apply triple barrier labeling
            labels = []
            for i in range(len(data)):
                if i >= len(data) - 1:  # Skip last point
                    labels.append(0)
                    continue
                
                entry_price = data.iloc[i]["close"]
                current_volatility = volatility.iloc[i] if not pd.isna(volatility.iloc[i]) else 0.01
                
                # Calculate barriers
                profit_take = entry_price * (1 + profit_take_multiplier * current_volatility)
                stop_loss = entry_price * (1 - stop_loss_multiplier * current_volatility)
                
                # Check if barriers are hit within time limit
                label = 0  # Default to no signal
                
                for j in range(i + 1, min(i + time_barrier_periods + 1, len(data))):
                    high_price = data.iloc[j]["high"]
                    low_price = data.iloc[j]["low"]
                    
                    if high_price >= profit_take:
                        label = 1  # Profit take hit
                        break
                    elif low_price <= stop_loss:
                        label = -1  # Stop loss hit
                        break
                
                labels.append(label)
            
            # Add labels to data
            labeled_data['label'] = labels
            
            return labeled_data
            
        except Exception as e:
            self.logger.error(f"âŒ Error in Analyst Labeling and Feature Engineering: {e}")
            raise


class AutoencoderConfig:
    """Configuration class for autoencoder models."""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path
        self.logger = system_logger.getChild("AutoencoderConfig")
        
    def load_config(self) -> Dict[str, Any]:
        """Load autoencoder configuration."""
        try:
            if self.config_path and os.path.exists(self.config_path):
                import yaml
                with open(self.config_path, 'r') as f:
                    return yaml.safe_load(f)
            else:
                self.logger.error(f"Error loading config: expected str, bytes or os.PathLike object, not dict, using default config.")
                return self._get_default_config()
        except Exception as e:
            self.logger.error(f"Error loading config: {e}, using default config.")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default autoencoder configuration."""
        return {
            "input_dim": 50,
            "hidden_dim": 25,
            "latent_dim": 10,
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 100
        }


class PrecomputedFeaturesManager:
    """Manager for precomputed features."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = system_logger.getChild("PrecomputedFeaturesManager")
        
        # Check for InfluxDB availability
        try:
            from influxdb_client import InfluxDBClient
            self.influxdb_available = True
        except ImportError:
            self.influxdb_available = False
            self.logger.warning("InfluxDB not available - features will be stored locally only")
    
    def store_features(self, features: pd.DataFrame, metadata: Dict[str, Any]) -> bool:
        """Store features with metadata."""
        try:
            if self.influxdb_available:
                # Store in InfluxDB
                return self._store_in_influxdb(features, metadata)
            else:
                # Store locally
                return self._store_locally(features, metadata)
        except Exception as e:
            self.logger.error(f"Error storing features: {e}")
            return False
    
    def _store_locally(self, features: pd.DataFrame, metadata: Dict[str, Any]) -> bool:
        """Store features locally."""
        try:
            # Create storage directory
            storage_dir = "data/features"
            os.makedirs(storage_dir, exist_ok=True)
            
            # Save features
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"features_{timestamp}.parquet"
            filepath = os.path.join(storage_dir, filename)
            
            features.to_parquet(filepath)
            
            # Save metadata
            metadata_file = filepath.replace('.parquet', '_metadata.json')
            import json
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f)
            
            return True
        except Exception as e:
            self.logger.error(f"Error storing features locally: {e}")
            return False
    
    def _store_in_influxdb(self, features: pd.DataFrame, metadata: Dict[str, Any]) -> bool:
        """Store features in InfluxDB."""
        # This would be implemented if InfluxDB is available
        return False


class EfficientFeaturesDatabase:
    """Database for efficient feature storage and retrieval."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = system_logger.getChild("EfficientFeaturesDatabase")
        self.databases = {}
    
    def has_existing_database(self) -> bool:
        """Check if there's an existing database."""
        # Check for existing databases
        existing_dbs = self._find_existing_databases()
        self.logger.info(f"Found {len(existing_dbs)} existing precomputed features databases")
        return len(existing_dbs) > 0
    
    def _find_existing_databases(self) -> List[str]:
        """Find existing feature databases."""
        # This would scan for existing databases
        return []
    
    def get_missing_time_ranges(self, historical_data: pd.DataFrame) -> List[tuple]:
        """Get missing time ranges that need processing."""
        # For now, return the entire range
        if len(historical_data) == 0:
            return []
        
        start_time = historical_data['timestamp'].min()
        end_time = historical_data['timestamp'].max()
        
        return [(start_time, end_time)]
    
    def store_features(
        self, 
        features: pd.DataFrame, 
        start_time: datetime, 
        end_time: datetime, 
        symbol: str, 
        exchange: str
    ) -> bool:
        """Store features in the database."""
        try:
            # Create storage directory
            storage_dir = f"data/features/{exchange}_{symbol}"
            os.makedirs(storage_dir, exist_ok=True)
            
            # Save features
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"features_{start_time.strftime('%Y%m%d_%H%M%S')}_{end_time.strftime('%Y%m%d_%H%M%S')}_{timestamp}.parquet"
            filepath = os.path.join(storage_dir, filename)
            
            features.to_parquet(filepath)
            
            return True
        except Exception as e:
            self.logger.error(f"Error storing features: {e}")
            return False


class MetaLabelingSystem:
    """Meta-labeling system for enhanced labeling."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = system_logger.getChild("MetaLabelingSystem")
    
    async def initialize(self) -> None:
        """Initialize the meta-labeling system."""
        self.logger.info("ðŸš€ Initializing meta-labeling system...")
        self.logger.info("âœ… Meta-labeling system initialized successfully")
    
    async def apply_meta_labeling(self, data: pd.DataFrame) -> pd.DataFrame:
        """Apply meta-labeling to the data."""
        try:
            # This would implement meta-labeling logic
            return data
        except Exception as e:
            self.logger.error(f"Error in meta-labeling: {e}")
            return data
