import asyncio
import json
import os
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

# Add the project root to the Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backtesting.ares_deep_analyzer import (
    run_walk_forward_analysis,
)
from src.config import CONFIG  # Import CONFIG
from src.utils.error_handler import (
    handle_errors,
    handle_file_operations,
    handle_specific_errors,
)
from src.utils.logger import setup_logging, system_logger


class WalkForwardValidationStep:
    """
    Enhanced walk forward validation step with comprehensive error handling and type safety.
    """

    def __init__(self, config: dict[str, Any]) -> None:
        """
        Initialize walk forward validation step with enhanced type safety.

        Args:
            config: Configuration dictionary
        """
        self.config: dict[str, Any] = config
        self.logger = system_logger.getChild("WalkForwardValidationStep")

        # Validation state
        self.is_validating: bool = False
        self.validation_progress: float = 0.0
        self.last_validation_time: datetime | None = None
        self.validation_history: list[dict[str, Any]] = []

        # Configuration
        self.step_config: dict[str, Any] = self.config.get(
            "step6_walk_forward_validation",
            {},
        )
        self.data_dir: str = self.step_config.get("data_directory", "data")
        self.models_dir: str = self.step_config.get("models_directory", "models")
        self.results_dir: str = self.step_config.get("results_directory", "results")
        self.n_splits: int = self.step_config.get("n_splits", 5)

    @handle_specific_errors(
        error_handlers={
            ValueError: (False, "Invalid walk forward validation configuration"),
            AttributeError: (False, "Missing required validation parameters"),
            KeyError: (False, "Missing configuration keys"),
        },
        default_return=False,
        context="walk forward validation initialization",
    )
    async def initialize(self) -> bool:
        """
        Initialize walk forward validation step with enhanced error handling.

        Returns:
            bool: True if initialization successful, False otherwise
        """
        try:
            self.logger.info("Initializing Walk Forward Validation Step...")

            # Load validation configuration
            await self._load_validation_configuration()

            # Validate configuration
            if not self._validate_configuration():
                self.logger.error("Invalid configuration for walk forward validation")
                return False

            # Initialize directories
            await self._initialize_directories()

            self.logger.info(
                "âœ… Walk Forward Validation Step initialization completed successfully",
            )
            return True

        except Exception as e:
            self.logger.error(
                f"âŒ Walk Forward Validation Step initialization failed: {e}",
            )
            return False

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="validation configuration loading",
    )
    async def _load_validation_configuration(self) -> None:
        """Load validation configuration."""
        try:
            # Set default validation parameters
            self.step_config.setdefault("data_directory", "data")
            self.step_config.setdefault("models_directory", "models")
            self.step_config.setdefault("results_directory", "results")
            self.step_config.setdefault("n_splits", 5)

            # Update configuration
            self.data_dir = self.step_config["data_directory"]
            self.models_dir = self.step_config["models_directory"]
            self.results_dir = self.step_config["results_directory"]
            self.n_splits = self.step_config["n_splits"]

            self.logger.info("Validation configuration loaded successfully")

        except Exception as e:
            self.logger.error(f"Error loading validation configuration: {e}")

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=False,
        context="configuration validation",
    )
    def _validate_configuration(self) -> bool:
        """
        Validate validation configuration.

        Returns:
            bool: True if configuration is valid, False otherwise
        """
        try:
            # Validate data directory
            if not self.data_dir or not os.path.exists(self.data_dir):
                self.logger.error("Invalid data directory")
                return False

            # Validate models directory
            if not self.models_dir:
                self.logger.error("Invalid models directory")
                return False

            # Validate results directory
            if not self.results_dir:
                self.logger.error("Invalid results directory")
                return False

            # Validate validation parameters
            if self.n_splits <= 0:
                self.logger.error("Invalid number of splits")
                return False

            self.logger.info("Configuration validation successful")
            return True

        except Exception as e:
            self.logger.error(f"Error validating configuration: {e}")
            return False

    @handle_file_operations(
        default_return=None,
        context="directory initialization",
    )
    async def _initialize_directories(self) -> None:
        """Initialize directories."""
        try:
            # Create results directory
            if not os.path.exists(self.results_dir):
                os.makedirs(self.results_dir, exist_ok=True)
                self.logger.info(f"Created results directory: {self.results_dir}")

            # Create subdirectories
            subdirs = ["walk_forward_validation", "intermediate_results", "logs"]
            for subdir in subdirs:
                subdir_path = os.path.join(self.results_dir, subdir)
                if not os.path.exists(subdir_path):
                    os.makedirs(subdir_path, exist_ok=True)
                    self.logger.info(f"Created subdirectory: {subdir_path}")

            self.logger.info("Directories initialized successfully")

        except Exception as e:
            self.logger.error(f"Error initializing directories: {e}")

    @handle_specific_errors(
        error_handlers={
            ValueError: (False, "Invalid validation parameters"),
            AttributeError: (False, "Missing validation components"),
            KeyError: (False, "Missing required data"),
        },
        default_return=False,
        context="walk forward validation execution",
    )
    async def execute(self) -> bool:
        """
        Execute walk forward validation with enhanced error handling.

        Returns:
            bool: True if validation successful, False otherwise
        """
        try:
            self.logger.info("Starting walk forward validation...")

            self.is_validating = True
            self.validation_progress = 0.0

            # Load training data
            training_data = await self._load_training_data()
            if training_data is None:
                self.logger.error("Failed to load training data")
                return False

            # Load trained model
            trained_model = await self._load_trained_model()
            if trained_model is None:
                self.logger.error("Failed to load trained model")
                return False

            # Perform walk forward validation
            validation_result = await self._perform_walk_forward_validation(
                training_data,
                trained_model,
            )
            if validation_result is None:
                self.logger.error("Failed to perform walk forward validation")
                return False

            # Save validation results
            await self._save_validation_results(validation_result)

            # Update validation state
            self.is_validating = False
            self.last_validation_time = datetime.now()

            # Record validation history
            self.validation_history.append(
                {
                    "timestamp": self.last_validation_time,
                    "n_splits": self.n_splits,
                    "result": validation_result,
                },
            )

            self.logger.info("âœ… Walk forward validation completed successfully")
            return True

        except Exception as e:
            self.logger.error(f"âŒ Walk forward validation failed: {e}")
            self.is_validating = False
            return False

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="training data loading",
    )
    async def _load_training_data(self) -> pd.DataFrame | None:
        """
        Load training data for validation.

        Returns:
            Optional[pd.DataFrame]: Training data or None if failed
        """
        try:
            # Look for consolidated data files
            data_files = []
            for file in os.listdir(self.data_dir):
                if file.endswith("_consolidated.csv"):
                    data_files.append(os.path.join(self.data_dir, file))

            if not data_files:
                self.logger.error("No consolidated data files found")
                return None

            # Load the first available data file
            data_file = data_files[0]
            self.logger.info(f"Loading training data from: {data_file}")

            # Load data
            data = pd.read_csv(data_file)

            # Basic data validation
            if data.empty:
                self.logger.error("Training data is empty")
                return None

            required_columns = ["timestamp", "open", "high", "low", "close", "volume"]
            missing_columns = [
                col for col in required_columns if col not in data.columns
            ]

            if missing_columns:
                self.logger.error(f"Missing required columns: {missing_columns}")
                return None

            self.logger.info(f"Loaded training data with {len(data)} rows")
            return data

        except Exception as e:
            self.logger.error(f"Error loading training data: {e}")
            return None

    @handle_file_operations(
        default_return=None,
        context="trained model loading",
    )
    async def _load_trained_model(self) -> dict[str, Any] | None:
        """
        Load trained model for validation.

        Returns:
            Optional[Dict[str, Any]]: Trained model or None if not found
        """
        try:
            # Look for trained model files
            model_files = []
            for file in os.listdir(self.models_dir):
                if file.endswith(".joblib") or file.endswith(".pkl"):
                    model_files.append(os.path.join(self.models_dir, file))

            if not model_files:
                self.logger.warning("No trained model files found")
                return None

            # Load the first available model file
            model_file = model_files[0]
            self.logger.info(f"Loading trained model from: {model_file}")

            # Simulate model loading
            model_info = {
                "model_path": model_file,
                "model_type": "neural_network",
                "model_size": os.path.getsize(model_file),
                "load_time": datetime.now(),
            }

            return model_info

        except Exception as e:
            self.logger.error(f"Error loading trained model: {e}")
            return None

    @handle_errors(
        exceptions=(ValueError, AttributeError),
        default_return=None,
        context="walk forward validation execution",
    )
    async def _perform_walk_forward_validation(
        self,
        data: pd.DataFrame,
        trained_model: dict[str, Any],
    ) -> dict[str, Any] | None:
        """
        Perform walk forward validation.

        Args:
            data: Training data DataFrame
            trained_model: Trained model information

        Returns:
            Optional[Dict[str, Any]]: Validation results or None if failed
        """
        try:
            # Simulate validation process
            await asyncio.sleep(3)  # Simulate processing time

            # Generate sample validation results
            validation_result = {
                "n_splits": self.n_splits,
                "total_samples": len(data),
                "splits": [],
                "overall_metrics": {
                    "accuracy": np.random.uniform(0.65, 0.85),
                    "precision": np.random.uniform(0.60, 0.80),
                    "recall": np.random.uniform(0.55, 0.75),
                    "f1_score": np.random.uniform(0.60, 0.80),
                    "sharpe_ratio": np.random.uniform(0.5, 2.0),
                    "max_drawdown": np.random.uniform(0.05, 0.25),
                },
                "validation_time": np.random.uniform(30, 120),
            }

            # Generate results for each split
            for i in range(self.n_splits):
                split_result = {
                    "split": i + 1,
                    "train_size": int(len(data) * 0.7),
                    "test_size": int(len(data) * 0.3),
                    "accuracy": np.random.uniform(0.60, 0.90),
                    "precision": np.random.uniform(0.55, 0.85),
                    "recall": np.random.uniform(0.50, 0.80),
                    "f1_score": np.random.uniform(0.55, 0.85),
                    "profit_factor": np.random.uniform(0.8, 2.5),
                    "win_rate": np.random.uniform(0.45, 0.75),
                }
                validation_result["splits"].append(split_result)

            self.logger.info(
                f"Walk forward validation completed with overall accuracy: {validation_result['overall_metrics']['accuracy']:.4f}",
            )
            return validation_result

        except Exception as e:
            self.logger.error(f"Error performing walk forward validation: {e}")
            return None

    @handle_file_operations(
        default_return=None,
        context="validation results saving",
    )
    async def _save_validation_results(self, results: dict[str, Any]) -> None:
        """
        Save validation results to file.

        Args:
            results: Validation results dictionary
        """
        try:
            # Create results filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = os.path.join(
                self.results_dir,
                "walk_forward_validation",
                f"walk_forward_validation_{timestamp}.json",
            )

            # Save results
            import json

            with open(results_file, "w") as f:
                json.dump(results, f, indent=2, default=str)

            self.logger.info(f"Validation results saved to: {results_file}")

        except Exception as e:
            self.logger.error(f"Error saving validation results: {e}")

    def get_validation_status(self) -> dict[str, Any]:
        """
        Get validation status information.

        Returns:
            Dict[str, Any]: Validation status
        """
        return {
            "is_validating": self.is_validating,
            "validation_progress": self.validation_progress,
            "last_validation_time": self.last_validation_time,
            "n_splits": self.n_splits,
            "data_directory": self.data_dir,
            "models_directory": self.models_dir,
            "results_directory": self.results_dir,
            "validation_history_count": len(self.validation_history),
        }

    @handle_errors(
        exceptions=(Exception,),
        default_return=None,
        context="walk forward validation cleanup",
    )
    async def stop(self) -> None:
        """Stop the walk forward validation step."""
        self.logger.info("ðŸ›‘ Stopping Walk Forward Validation Step...")

        try:
            # Stop validation if running
            if self.is_validating:
                self.is_validating = False
                self.logger.info("Walk forward validation stopped")

            self.logger.info("âœ… Walk Forward Validation Step stopped successfully")

        except Exception as e:
            self.logger.error(f"Error stopping walk forward validation: {e}")


@handle_errors(
    exceptions=(Exception,),
    default_return=False,
    context="walk_forward_validation_step",
)
async def run_step(symbol: str, data_dir: str, timeframe: str = "1m", exchange: str = "BINANCE") -> bool:
    """
    Runs walk-forward validation.
    Loads data from the specified pickle file.
    """
    setup_logging()
    logger = system_logger.getChild("Step6WalkForwardValidation")

    start_time = time.time()
    logger.info("=" * 80)
    logger.info("ðŸš€ STEP 6: WALK-FORWARD VALIDATION")
    logger.info("=" * 80)
    logger.info(f"ðŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ðŸŽ¯ Symbol: {symbol}")
    logger.info(f"ðŸ“ Data directory: {data_dir}")

    # Check if this is a blank training run and override configuration accordingly
    blank_training_mode = os.environ.get("BLANK_TRAINING_MODE", "0") == "1"
    if blank_training_mode:
        logger.info(
            "ðŸ”§ BLANK TRAINING MODE DETECTED - Overriding configuration for faster execution",
        )
        # Reduce validation parameters for blank training mode
        n_splits = 3  # Reduced from default 5
        test_size = 0.2  # Reduced test size
        logger.info(
            f"ðŸ”§ BLANK TRAINING MODE: Using reduced parameters (n_splits={n_splits}, test_size={test_size})",
        )
    else:
        n_splits = 5  # Default
        test_size = 0.3  # Default

    try:
        # Step 6.1: Load PREPARED data from the file saved in step 4
        logger.info("ðŸ“Š STEP 6.1: Loading data from pickle file...")
        data_load_start = time.time()

        # Enhanced data path resolution with extensive logging
        logger.info("ðŸ” DATA PATH RESOLUTION:")
        logger.info(f"   - Data directory: {data_dir}")
        logger.info(f"   - Symbol: {symbol}")
        logger.info(f"   - Exchange: {exchange}")
        
        # Try multiple possible paths
        possible_paths = [
            os.path.join(data_dir, f"{exchange}_{symbol}_prepared_data.pkl"),
            os.path.join(data_dir, f"{symbol}_prepared_data.pkl"),
            os.path.join(data_dir, f"BINANCE_{symbol}_prepared_data.pkl"),
            os.path.join(data_dir, f"{symbol}_prepared_data.pkl")
        ]
        
        logger.info("   ðŸ“ Checking possible prepared data paths:")
        for i, path in enumerate(possible_paths, 1):
            exists = os.path.exists(path)
            size = os.path.getsize(path) if exists else 0
            logger.info(f"      {i}. {path} - {'âœ… EXISTS' if exists else 'âŒ NOT FOUND'} ({size} bytes)")
        
        # Use the first existing path or the original path
        prepared_data_path = None
        for path in possible_paths:
            if os.path.exists(path):
                prepared_data_path = path
                logger.info(f"   âœ… Using prepared data path: {prepared_data_path}")
                break
        
        if not prepared_data_path:
            logger.warning(f"âš ï¸  No prepared data file found in any expected location")
            logger.info("ðŸ”§ Creating fallback prepared data from historical data...")

            # Load historical data as fallback
            historical_data_path = os.path.join(
                data_dir,
                f"{exchange}_{symbol}_historical_data.pkl",
            )
            if os.path.exists(historical_data_path):
                with open(historical_data_path, "rb") as f:
                    historical_data = pickle.load(f)

                # Use klines data as prepared data
                prepared_data = historical_data["klines"]

                # Add basic features for validation
                if not prepared_data.empty:
                    # Add basic technical indicators
                    prepared_data["sma_20"] = (
                        prepared_data["close"].rolling(window=20).mean()
                    )
                    prepared_data["rsi"] = 100 - (
                        100
                        / (
                            1
                            + (
                                prepared_data["close"].diff().rolling(window=14).mean()
                                / prepared_data["close"].diff().rolling(window=14).std()
                            )
                        )
                    )
                    prepared_data["volume_sma"] = (
                        prepared_data["volume"].rolling(window=20).mean()
                    )

                    # Drop NaN values
                    prepared_data = prepared_data.dropna()

                    logger.info(
                        f"âœ… Created fallback prepared data: {prepared_data.shape}",
                    )
                else:
                    logger.error("âŒ Historical data is empty")
                    return False
            else:
                logger.error(
                    f"âŒ Historical data file not found: {historical_data_path}",
                )
                return False
        else:
            with open(prepared_data_path, "rb") as f:
                prepared_data = pickle.load(f)

        data_load_duration = time.time() - data_load_start
        logger.info(f"â±ï¸  Data loading completed in {data_load_duration:.2f} seconds")
        logger.info(
            f"ðŸ“Š Loaded prepared data: {len(prepared_data)} rows, {len(prepared_data.columns)} columns",
        )

        if prepared_data.empty:
            logger.error("âŒ Prepared data is empty. Aborting.")
            return False

        # Step 6.2: Load best parameters
        logger.info("ðŸŽ¯ STEP 6.4: Loading best parameters...")
        params_load_start = time.time()

        best_params = CONFIG.get(
            "best_params",
            {},
        )  # Use the globally updated best_params

        params_load_duration = time.time() - params_load_start
        logger.info(
            f"â±ï¸  Parameters loading completed in {params_load_duration:.2f} seconds",
        )
        logger.info("âœ… Loaded best parameters:")
        for param, value in best_params.items():
            logger.info(f"   - {param}: {value}")

        # Step 6.3: Run walk-forward analysis
        logger.info("ðŸ”„ STEP 6.6: Running walk-forward analysis...")
        wfa_start = time.time()

        logger.info("ðŸ”¢ Walk-forward analysis parameters:")
        logger.info(f"   - Symbol: {symbol}")
        logger.info(f"   - Data shape: {prepared_data.shape}")
        logger.info(f"   - Best params count: {len(best_params)}")

        wfa_report_str = await run_walk_forward_analysis(prepared_data, best_params)

        wfa_duration = time.time() - wfa_start
        logger.info(f"â±ï¸  Walk-forward analysis completed in {wfa_duration:.2f} seconds")
        logger.info(f"ðŸ“Š Analysis report length: {len(wfa_report_str)} characters")

        # Step 6.4: Save reports
        logger.info("ðŸ’¾ STEP 6.7: Saving reports...")
        save_start = time.time()

        reports_dir = os.path.join(
            Path(__file__).parent.parent.parent,
            "reports",
        )  # Access reports dir
        os.makedirs(reports_dir, exist_ok=True)
        wfa_file = os.path.join(reports_dir, f"{symbol}_walk_forward_report.txt")

        with open(wfa_file, "w") as f:
            f.write(wfa_report_str)

        # Parse metrics from the report string
        metrics = _parse_report_for_metrics(wfa_report_str)

        # Save metrics to a JSON file for the TrainingManager
        output_metrics_file = os.path.join(data_dir, f"{symbol}_wfa_metrics.json")
        with open(output_metrics_file, "w") as f:
            json.dump(metrics, f, indent=4)

        save_duration = time.time() - save_start
        logger.info(f"â±ï¸  Reports saved in {save_duration:.2f} seconds")
        logger.info("ðŸ“„ Reports saved to:")
        logger.info(f"   - Walk-forward report: {wfa_file}")
        logger.info(f"   - Metrics JSON: {output_metrics_file}")
        logger.info("ðŸ“Š Extracted metrics:")
        for metric, value in metrics.items():
            logger.info(f"   - {metric}: {value}")

        # Run backtesting quality analysis with extensive logging
        logger.info("ðŸ” Running backtesting quality analysis...")
        print("ðŸ” Running backtesting quality analysis...")

        try:
            # Enhanced import analysis with detailed logging
            logger.info("   ðŸ” IMPORT ANALYSIS:")
            logger.info("      - Checking Python path...")
            
            import sys
            logger.info(f"      - Python version: {sys.version}")
            logger.info(f"      - Python path length: {len(sys.path)}")
            
            # Check seaborn availability
            logger.info("      - Checking seaborn availability...")
            try:
                import seaborn as sns
                logger.info(f"      âœ… Seaborn imported successfully - version: {sns.__version__}")
            except ImportError as e:
                logger.warning(f"      âŒ Seaborn import failed: {e}")
                logger.warning("      - Attempting to install seaborn...")
                try:
                    import subprocess
                    result = subprocess.run([sys.executable, "-m", "pip", "install", "seaborn"], 
                                         capture_output=True, text=True)
                    if result.returncode == 0:
                        logger.info("      âœ… Seaborn installed successfully")
                        import seaborn as sns
                        logger.info(f"      âœ… Seaborn imported after installation - version: {sns.__version__}")
                    else:
                        logger.warning(f"      âŒ Seaborn installation failed: {result.stderr}")
                except Exception as install_error:
                    logger.warning(f"      âŒ Seaborn installation error: {install_error}")
            
            # Check other required packages
            logger.info("      - Checking other required packages...")
            try:
                import matplotlib
                logger.info(f"      âœ… Matplotlib available - version: {matplotlib.__version__}")
            except ImportError as e:
                logger.warning(f"      âŒ Matplotlib import failed: {e}")
            
            try:
                import pandas as pd
                logger.info(f"      âœ… Pandas available - version: {pd.__version__}")
            except ImportError as e:
                logger.warning(f"      âŒ Pandas import failed: {e}")
            
            # Import the quality analyzer with enhanced error handling
            logger.info("      - Importing quality analyzer...")
            sys.path.append(str(Path(__file__).parent.parent.parent.parent))
            logger.info(f"      - Added path: {Path(__file__).parent.parent.parent.parent}")
            
            try:
                from analysis.backtesting_quality_analysis import BacktestingQualityAnalyzer
                logger.info("      âœ… BacktestingQualityAnalyzer imported successfully")
            except ImportError as e:
                logger.error(f"      âŒ Failed to import BacktestingQualityAnalyzer: {e}")
                raise

            # Create analyzer and prepare backtest data with logging
            logger.info("   ðŸ¤– CREATING QUALITY ANALYZER:")
            analyzer = BacktestingQualityAnalyzer()
            logger.info("      âœ… Analyzer created successfully")

            # Create backtest results DataFrame from the metrics
            logger.info("   ðŸ“Š PREPARING BACKTEST DATA:")
            logger.info(f"      - Metrics keys: {list(metrics.keys())}")
            logger.info(f"      - Metrics values: {metrics}")
            
            backtest_data = pd.DataFrame([metrics])
            logger.info(f"      - Backtest data shape: {backtest_data.shape}")
            logger.info(f"      - Backtest data columns: {list(backtest_data.columns)}")

            analyzer.backtest_data = backtest_data
            logger.info("      âœ… Backtest data assigned to analyzer")

            # Run the analysis with timing
            logger.info("   ðŸ”¬ RUNNING QUALITY ANALYSIS:")
            analysis_start = time.time()
            analyzer.analyze_backtest_quality()
            analysis_time = time.time() - analysis_start
            logger.info(f"      âœ… Analysis completed in {analysis_time:.2f} seconds")

            # Save the quality report
            logger.info("   ðŸ’¾ SAVING QUALITY REPORT:")
            quality_report_path = f"{data_dir}/{symbol}_backtesting_quality_report.txt"
            logger.info(f"      - Report path: {quality_report_path}")
            
            save_start = time.time()
            analyzer.save_report(quality_report_path)
            save_time = time.time() - save_start
            logger.info(f"      âœ… Report saved in {save_time:.2f} seconds")

            logger.info(
                f"âœ… Backtesting quality analysis completed and saved to: {quality_report_path}",
            )
            print(
                f"âœ… Backtesting quality analysis completed and saved to: {quality_report_path}",
            )

        except Exception as e:
            logger.warning(f"âš ï¸  Backtesting quality analysis failed: {e}")
            logger.warning(f"   - Error type: {type(e).__name__}")
            logger.warning(f"   - Error details: {str(e)}")
            print(f"âš ï¸  Backtesting quality analysis failed: {e}")

        # Final summary
        total_duration = time.time() - start_time
        logger.info("=" * 80)
        logger.info("ðŸŽ‰ STEP 6: WALK-FORWARD VALIDATION COMPLETE")
        logger.info("=" * 80)
        logger.info(f"â±ï¸  Total duration: {total_duration:.2f} seconds")
        logger.info("ðŸ“„ Output files:")
        logger.info(f"   - Walk-forward report: {wfa_file}")
        logger.info(f"   - Metrics JSON: {output_metrics_file}")
        logger.info("âœ… Success: True")

        return True
    except Exception as e:
        total_duration = time.time() - start_time
        logger.error("=" * 80)
        logger.error("âŒ STEP 6: WALK-FORWARD VALIDATION FAILED")
        logger.error("=" * 80)
        logger.error(f"â±ï¸  Duration before failure: {total_duration:.2f} seconds")
        logger.error(f"ðŸ’¥ Error: {e}")
        logger.error("ðŸ“‹ Full traceback:", exc_info=True)
        return False


def _parse_report_for_metrics(report_content: str) -> dict[str, float]:
    """Parses a text report to extract key-value metrics."""
    import re

    metrics = {}
    # Adjusted patterns to match the output format of calculate_detailed_metrics and run_walk_forward_analysis
    patterns = {
        "Final Equity": r"Final Equity:\s*\$([0-9,]+\.\d{2})",
        "Total Trades": r"Total Trades:\s*(\d+)",
        "Sharpe Ratio": r"Sharpe Ratio:\s*(-?\d+\.\d{2})",
        "Sortino Ratio": r"Sortino Ratio:\s*(-?\d+\.\d{2})",
        "Max Drawdown (%)": r"Max Drawdown \(%\):\s*(-?\d+\.\d{2})",
        "Calmar Ratio": r"Calmar Ratio:\s*(-?\d+\.\d{2})",
        "Win Rate (%)": r"Win Rate \(%\):\s*(\d+\.\d{2})",
        "Profit Factor": r"Profit Factor:\s*(\d+\.\d{2})",
    }
    for key, pattern in patterns.items():
        match = re.search(pattern, report_content)
        if match:
            try:
                # Remove commas from numbers if present (e.g., $10,000.00)
                value = match.group(1).replace(",", "")
                metrics[key.replace(" ", "_").replace("%", "Pct").replace(".", "")] = (
                    float(value)
                )
            except (ValueError, IndexError):
                continue
    return metrics


if __name__ == "__main__":
    # Command-line arguments: symbol, data_dir
    symbol = sys.argv[1]
    data_dir = sys.argv[2]

    success = asyncio.run(run_step(symbol, data_dir))

    if not success:
        sys.exit(1)  # Indicate failure
    sys.exit(0)  # Indicate success
